'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/quick-start-guide/','title':"Quick Start Guide",'section':"Docs",'content':"Quick Start #  If you have a specific BurmillaOS machine requirements, please check out our guides on running BurmillaOS. With the rest of this guide, we\u0026rsquo;ll start up a BurmillaOS using Docker machine and show you some of what BurmillaOS can do.\nLaunching BurmillaOS using Docker Machine #  Before moving forward, you\u0026rsquo;ll need to have Docker Machine and VirtualBox installed. Once you have VirtualBox and Docker Machine installed, it\u0026rsquo;s just one command to get BurmillaOS running.\n$ docker-machine create -d virtualbox \\  --virtualbox-boot2docker-url https://github.com/burmilla/os/releases/download/\u0026lt;version\u0026gt;/burmillaos.iso \\  --virtualbox-memory 2048 \\  \u0026lt;MACHINE-NAME\u0026gt; That\u0026rsquo;s it! You\u0026rsquo;re up and running a BurmillaOS instance.\nTo log into the instance, just use the docker-machine command.\n$ docker-machine ssh \u0026lt;MACHINE-NAME\u0026gt; A First Look At BurmillaOS #  There are two Docker daemons running in BurmillaOS. The first is called System Docker, which is where BurmillaOS runs system services like ntpd and syslog. You can use the system-docker command to control the System Docker daemon.\nThe other Docker daemon running on the system is Docker, which can be accessed by using the normal docker command.\nWhen you first launch BurmillaOS, there are no containers running in the Docker daemon. However, if you run the same command against the System Docker, you’ll see a number of system services that are shipped with BurmillaOS.\n Note: system-docker can only be used by root, so it is necessary to use the sudo command whenever you want to interact with System Docker.\n $ sudo system-docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 07135915b03a burmilla/os-docker:19.03.13 \u0026#34;ros user-docker\u0026#34; 15 hours ago Up 15 hours docker 896c6169c2d5 burmilla/os-console:v2.0.0 \u0026#34;/usr/bin/ros entr...\u0026#34; 41 hours ago Up 24 hours console 74e57e5940de burmilla/os-base:v2.0.0 \u0026#34;/usr/bin/ros entr...\u0026#34; 41 hours ago Up 24 hours ntp 989e8f137fb7 burmilla/os-base:v2.0.0 \u0026#34;/usr/bin/ros entr...\u0026#34; 41 hours ago Up 24 hours network 79b750fa577a burmilla/os-base:v2.0.0 \u0026#34;/usr/bin/ros entr...\u0026#34; 41 hours ago Up 24 hours udev 29c582619c67 burmilla/container-crontab:v0.5.0 \u0026#34;container-crontab\u0026#34; 41 hours ago Up 24 hours system-cron cdd49fa26ecb burmilla/os-syslog:v2.0.0 \u0026#34;/usr/bin/entrypoi...\u0026#34; 41 hours ago Up 24 hours syslog e490108ce8da burmilla/os-acpid:v2.0.0 \u0026#34;/usr/bin/ros entr...\u0026#34; 41 hours ago Up 24 hours acpid Some containers are run at boot time, and others, such as the console, docker, etc. containers are always running.\nUsing BurmillaOS #  Deploying a Docker Container #  Let\u0026rsquo;s try to deploy a normal Docker container on the Docker daemon. The BurmillaOS Docker daemon is identical to any other Docker environment, so all normal Docker commands work.\n$ docker run -d nginx You can see that the nginx container is up and running:\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e99c2c4b8b30 nginx \u0026#34;nginx -g \u0026#39;daemon off\u0026#34; 12 seconds ago Up 11 seconds 80/tcp, 443/tcp drunk_ptolemy Deploying A System Service Container #  The following is a simple Docker container to set up Linux-dash, which is a minimal low-overhead web dashboard for monitoring Linux servers. The Dockerfile will be like this:\nFROMhwestphal/nodeboxMAINTAINERhussein.galal.ahmed.11@gmail.comRUN opkg-install unzipRUN curl -k -L -o master.zip https://github.com/afaqurk/linux-dash/archive/master.zipRUN unzip master.zipWORKDIRlinux-dash-masterRUN npm installENTRYPOINT [\u0026#34;node\u0026#34;,\u0026#34;server\u0026#34;]Using the hwestphal/nodebox image, which uses a Busybox image and installs node.js and npm. We downloaded the source code of Linux-dash, and then ran the server. Linux-dash will run on port 80 by default.\nTo run this container in System Docker use the following command:\n$ sudo system-docker run -d --net=host --name busydash husseingalal/busydash In the command, we used --net=host to tell System Docker not to containerize the container\u0026rsquo;s networking, and use the host’s networking instead. After running the container, you can see the monitoring server by accessing http://\u0026lt;IP_OF_MACHINE\u0026gt;.\n To make the container survive during the reboots, you can create the /opt/rancher/bin/start.sh script, and add the Docker start line to launch the Docker at each startup.\n$ sudo mkdir -p /opt/rancher/bin $ echo \u0026#34;sudo system-docker start busydash\u0026#34; | sudo tee -a /opt/rancher/bin/start.sh $ sudo chmod 755 /opt/rancher/bin/start.sh Using ROS #  Another useful command that can be used with BurmillaOS is ros which can be used to control and configure the system.\n$ sudo ros -v version v2.0.0 from os image burmilla/os:v2.0.0 BurmillaOS state is controlled by a cloud config file. ros is used to edit the configuration of the system, to see for example the dns configuration of the system:\n$ sudo ros config get rancher.network.dns.nameservers - 8.8.8.8 - 8.8.4.4 When using the native Busybox console, any changes to the console will be lost after reboots, only changes to /home or /opt will be persistent. You can use the ros console switch command to switch to a persistent console and replace the native Busybox console. For example, to switch to the Ubuntu console:\n$ sudo ros console switch ubuntu Conclusion #  BurmillaOS is a simple Linux distribution ideal for running Docker. By embracing containerization of system services and leveraging Docker for management, BurmillaOS hopes to provide a very reliable, and easy to manage OS for running containers.\n"});index.add({'id':1,'href':'/docs/storage/custom-partition-layout/','title':"Custom Partition Layout",'section':"Storage",'content':"How to custom partition layout #  When users use the default ros install, ROS will automatically create one partition on the root disk. It will be the only partition with the label RANCHER_STATE. But sometimes users want to be able to customize the root disk partition to isolate the data.\n The following defaults to MBR mode, GPT mode has not been tested.\n Partions #  RANCHER_STATE #  As mentioned above, the default mode is that ROS will automatically create one partition with the label RANCHER_STATE.\nIn addition, we can have other partitions, e.g.: two partitions, one is RANCHER_STATE and the other is a normal partition.\nFirst boot a ROS instance from ISO, then manually format and partition /dev/sda , the reference configuration is as follows:\n[root@burmilla oem]# fdisk -l Disk /dev/sda: 5 GiB, 5377622016 bytes, 10503168 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x9fff87e9 Device Boot Start End Sectors Size Id Type /dev/sda1 * 2048 7503167 7501120 3.6G 83 Linux /dev/sda2 7503872 10503167 2999296 1.4G 83 Linux [root@burmilla oem]# blkid /dev/sda1: LABEL=\u0026#34;RANCHER_STATE\u0026#34; UUID=\u0026#34;512f212b-3130-458e-a2d1-1d601c34d4e4\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;9fff87e9-01\u0026#34; /dev/sda2: UUID=\u0026#34;3828e3ac-b825-4898-9072-45da9d37c2a6\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;9fff87e9-02\u0026#34; Then install ROS to the disk with ros install -t noformat -d /dev/sda ....\nAfter rebooting, you can use /dev/sda2. For example, changing the data root of user-docker:\n$ ros config set mounts \u0026#39;[[\u0026#34;/dev/sda2\u0026#34;,\u0026#34;/mnt/s\u0026#34;,\u0026#34;ext4\u0026#34;,\u0026#34;\u0026#34;]]’ $ ros config set rancher.docker.graph /mnt/s $ reboot  In this mode, the RANCHER_STATE partition capacity cannot exceed 3.8GiB, otherwise the bootloader may not recognize the boot disk. This is the test result on VirtualBox.\n RANCHER_BOOT #  When you only use the RANCHER_STATE partition, the bootloader will be installed in the /boot directory.\n$ system-docker run -it --rm -v /:/host alpine ls /host/boot ... If you want to use a separate boot partition, you also need to boot a ROS instance from ISO, then manually format and partition /dev/sda:\n[root@burmilla burmilla]# fdisk -l Disk /dev/sda: 5 GiB, 5377622016 bytes, 10503168 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xe32b3025 Device Boot Start End Sectors Size Id Type /dev/sda1 2048 2503167 2501120 1.2G 83 Linux /dev/sda2 2504704 7503167 4998464 2.4G 83 Linux /dev/sda3 7503872 10503167 2999296 1.4G 83 Linux [root@burmilla burmilla]# mkfs.ext4 -L RANCHER_BOOT /dev/sda1 [root@burmilla burmilla]# mkfs.ext4 -L RANCHER_STATE /dev/sda2 [root@burmilla burmilla]# mkfs.ext4 /dev/sda3 [root@burmilla burmilla]# blkid /dev/sda1: LABEL=\u0026#34;RANCHER_BOOT\u0026#34; UUID=\u0026#34;43baeac3-11f3-4eed-acfa-64daf66b26c8\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;e32b3025-01\u0026#34; /dev/sda2: LABEL=\u0026#34;RANCHER_STATE\u0026#34; UUID=\u0026#34;16f1ecef-dbe4-42a2-87a1-611939684e0b\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;e32b3025-02\u0026#34; /dev/sda3: UUID=\u0026#34;9f34e161-0eee-48f9-93de-3b7c54dea437\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;c9b8f181-03\u0026#34; Then install ROS to the disk with ros install -t noformat -d /dev/sda ....\nAfter rebooting, you can check the boot partition:\n[root@burmilla burmilla]# mkdir /boot [root@burmilla burmilla]# mount /dev/sda1 /boot [root@burmilla burmilla]# ls -ahl /boot/ total 175388 drwxr-xr-x 4 root root 4.0K Sep 27 03:35 . drwxr-xr-x 1 root root 4.0K Sep 27 03:38 .. -rw-r--r-- 1 root root 24 Sep 27 03:05 append -rw-r--r-- 1 root root 128 Sep 27 03:35 global.cfg -rw-r--r-- 1 root root 96.8M Sep 27 03:05 initrd If you are not using the first partition as a BOOT partition, you need to set BOOT flag via the fdisk tool.\n In this mode, the RANCHER_BOOT partition capacity cannot exceed 3.8GiB, otherwise the bootloader may not recognize the boot disk. This is the test result on VirtualBox.\n BURMILLA_OEM #  If you format any partition with the label BURMILLA_OEM, ROS will mount this partition to /usr/share/ros/oem:\n[root@burmilla burmilla]# blkid /dev/sda2: LABEL=\u0026#34;BURMILLA_OEM\u0026#34; UUID=\u0026#34;4f438455-63a3-4d29-ac90-50adbeced412\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;9fff87e9-02\u0026#34; [root@burmilla burmilla]# df -hT | grep sda2 /dev/sda2 ext4 1.4G 4.3M 1.3G 0% /usr/share/ros/oem Currently, this OEM directory is hardcoded and not configurable.\nBURMILLA_SWAP #  Suppose you have a partition(/dev/sda2) and you want to use it as a SWAP partition:\n$ mkswap -L BURMILLA_SWAP /dev/sda2 $ blkid /dev/sda1: LABEL=\u0026#34;RANCHER_STATE\u0026#34; UUID=\u0026#34;512f212b-3130-458e-a2d1-1d601c34d4e4\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;9fff87e9-01\u0026#34; /dev/sda2: LABEL=\u0026#34;BURMILLA_SWAP\u0026#34; UUID=\u0026#34;772b6e76-f89c-458e-931e-10902d78d3e4\u0026#34; TYPE=\u0026#34;swap\u0026#34; PARTUUID=\u0026#34;9fff87e9-02\u0026#34; After you install ROS to the disk, you can add the runcmd to enable SWAP:\nruncmd: - swapon -L BURMILLA_SWAP Then check the memory information:\n[root@burmilla burmilla]# free -m total used free shared buffers cached Mem: 1996 774 1221 237 20 614 -/+ buffers/cache: 139 1856 Swap: 487 0 487 "});index.add({'id':2,'href':'/docs/installation/cloud/aws-ec2/','title':"Amazon EC2",'section':"Cloud",'content':"Amazon EC2 #  BurmillaOS is available as an Amazon Web Services AMI, and can be easily run on EC2. You can launch BurmillaOS either using the AWS Command Line Interface (CLI) or using the AWS console.\nLaunching BurmillaOS through the AWS CLI #  If you haven\u0026rsquo;t installed the AWS CLI, follow the instructions on the AWS CLI page to install the CLI and configure access key and secret keys.\nOnce you\u0026rsquo;ve installed your AWS CLI, use this command to launch an EC2 instance with the BurmillaOS AMI. You will need to know your SSH key name and security group name for the region that you are configured for. These can be found from the AWS console.\n Note: Check the BurmillaOS README for AMI names for each region. We support PV and HVM types of AMIs.\n $ aws ec2 run-instances --image-id ami-ID# --count 1 --instance-type t2.small --key-name MySSHKeyName --security-groups sg-name Your EC2 instance is now running BurmillaOS!\nLaunching BurmillaOS through the AWS Console #  Let’s walk through how to import and create a BurmillaOS on EC2 machine using the AWS console.\n First login to your AWS console, and go to the EC2 dashboard, click on Launch Instance:  Select the Community AMIs on the sidebar and search for BurmillaOS. Pick the latest version and click Select.  Go through the steps of creating the instance type through the AWS console. If you want to pass in a cloud-config file during boot of BurmillaOS, you\u0026rsquo;d pass in the file as User data by expanding the Advanced Details in Step 3: Configure Instance Details. You can pass in the data as text or as a file. After going through all the steps, you finally click on Launch, and either create a new key pair or choose an existing key pair to be used with the EC2 instance. If you have created a new key pair, download the key pair. If you have chosen an existing key pair, make sure you have the key pair accessible. Click on Launch Instances.  Your instance will be launching and you can click on View Instances to see it\u0026rsquo;s status. Your instance is now running!   Logging into BurmillaOS #  From a command line, log into the EC2 Instance. If you added ssh keys using a cloud-config, both those keys, and the one you selected in the AWS UI will be installed.\n$ ssh -i /Directory/of/MySSHKeyName.pem rancher@\u0026lt;ip-of-ec2-instance\u0026gt; If you have issues logging into BurmillaOS, try using this command to help debug the issue.\n$ ssh -v -i /Directory/of/MySSHKeyName.pem rancher@\u0026lt;ip-of-ec2-instance\u0026gt; Latest AMI Releases #  Please check the README in our BurmillaOS repository for our latest AMIs.\n"});index.add({'id':3,'href':'/docs/installation/cloud/amazon-ecs/','title':"Amazon ECS",'section':"Cloud",'content':"Amazon ECS (EC2 Container Service) #   Amazon ECS is supported, which allows BurmillaOS EC2 instances to join your cluster.\nPre-Requisites #  Prior to launching BurmillaOS EC2 instances, the ECS Container Instance IAM Role will need to have been created. This ecsInstanceRole will need to be used when launching EC2 instances. If you have been using ECS, you created this role if you followed the ECS \u0026ldquo;Get Started\u0026rdquo; interactive guide.\nLaunching an instance with ECS #  BurmillaOS makes it easy to join your ECS cluster. The ECS agent is a system service that is enabled in the ECS enabled AMI. There may be other BurmillaOS AMIs that don\u0026rsquo;t have the ECS agent enabled by default, but it can easily be added in the user data on any BurmillaOS AMI.\nWhen launching the BurmillaOS AMI, you\u0026rsquo;ll need to specify the IAM Role and Advanced Details -\u0026gt; User Data in the Configure Instance Details step.\nFor the IAM Role, you\u0026rsquo;ll need to be sure to select the ECS Container Instance IAM role.\nFor the User Data, you\u0026rsquo;ll need to pass in the cloud-config file.\n#cloud-config rancher: environment: ECS_CLUSTER: your-ecs-cluster-name # Note: You will need to add this variable, if using awslogs for ECS task. ECS_AVAILABLE_LOGGING_DRIVERS: |- [\u0026#34;json-file\u0026#34;,\u0026#34;awslogs\u0026#34;] # If you have selected a BurmillaOS AMI that does not have ECS enabled by default, # you\u0026#39;ll need to enable the system service for the ECS agent. services_include: amazon-ecs-agent: true Version #  By default, the ECS agent will be using the latest tag for the amazon-ecs-agent image. In v0.5.0, we introduced the ability to select which version of the amazon-ecs-agent.\nTo select the version, you can update your cloud-config file.\n#cloud-config rancher: environment: ECS_CLUSTER: your-ecs-cluster-name # Note: You will need to make sure to include the colon in front of the version. ECS_AGENT_VERSION: :v2.0.0 # If you have selected a BurmillaOS AMI that does not have ECS enabled by default, # you\u0026#39;ll need to enable the system service for the ECS agent. services_include: amazon-ecs-agent: true  Note: The : must be in front of the version tag in order for the ECS image to be tagged correctly.\n "});index.add({'id':4,'href':'/docs/configuration/base/switching-consoles/','title':"Consoles",'section':"Base",'content':"Consoles #  When booting from the ISO, BurmillaOS starts with the default console, which is based on busybox.\nYou can select which console you want BurmillaOS to start with using the cloud-config.\nEnabling Consoles using Cloud-Config #  When launching BurmillaOS with a cloud-config file, you can select which console you want to use.\nCurrently, the list of available consoles are:\n default (debian)  Here is an example cloud-config file that can be used to enable the debian console.\n#cloud-config rancher: console: debian Listing Available Consoles #  You can easily list the available consoles in BurmillaOS and what their status is with sudo ros console list.\n$ sudo ros console list current default Console persistence #  The default console is persistent. Persistent console means that the console container will remain the same and preserves changes made to its filesystem across reboots. If a container is deleted/rebuilt, state in the console will be lost except what is in the persisted directories.\n/home /opt /var/lib/docker /var/lib/rancher  Note: When using a persistent console and in the current version\u0026rsquo;s console, rolling back is not supported. For example, rolling back to v0.4.5 when using a v0.5.0 persistent console is not supported.\n "});index.add({'id':5,'href':'/docs/configuration/base/date-and-timezone/','title':"Date and time zone",'section':"Base",'content':"Date and time zone #  Host system #  The default console keeps time in the Coordinated Universal Time (UTC) zone and synchronizes clocks with the Network Time Protocol (NTP). The Network Time Protocol daemon (ntpd) is an operating system program that maintains the system time in synchronization with time servers using the NTP.\nChanging the timezone in the default console can be for example done using a system-wide environment variable /etc/environment\n#cloud-config write_files: - path: /etc/environment content: | TZ=\u0026#34;Europe/Amsterdam\u0026#34; append: true BurmillaOS can run ntpd in the System Docker container. You can update its configurations by updating /etc/ntp.conf. For an example of how to update a file such as /etc/ntp.conf within a container, refer to this page.\nUsage in containers #  The host timezone is not used within containers and therefore needs to be set as an environment variable:\n$ docker run -e TZ=Europe/Amsterdam debian:jessie date Tue Aug 20 09:28:19 CEST 2019 You may need to install the tzdata in some images:\n$ docker run -e TZ=Asia/Shanghai -e DEBIAN_FRONTEND=noninteractive -it --rm ubuntu /bin/bash -c \u0026#34;apt-get update \u0026amp;\u0026amp; apt-get install -yq tzdata \u0026amp;\u0026amp; date” Thu Aug 29 08:13:02 CST 2019 "});index.add({'id':6,'href':'/docs/configuration/advanced/disable-access-to-system/','title':"Disabling Access to BurmillaOS",'section':"Advanced",'content':"Disabling Access to BurmillaOS #  In BurmillaOS, you can set rancher.password as a kernel parameter and auto-login to be enabled, but there may be some cases where we want to disable both of these options. Both of these options can be disabled in the cloud-config or as part of a ros command.\nHow to Disabling Options #  If BurmillaOS has already been started, you can use ros config set to update that you want to disable\n# Disabling the `rancher.password` kernel parameter $ sudo ros config set rancher.disable [\u0026#34;password\u0026#34;] # Disabling the `autologin` ability $ sudo ros config set rancher.disable [\u0026#34;autologin\u0026#34;] Alternatively, you can set it up in your cloud-config so it\u0026rsquo;s automatically disabled when you boot BurmillaOS.\n# cloud-config rancher: disable: - password - autologin "});index.add({'id':7,'href':'/docs/configuration/docker/images-prefix/','title':"Images prefix",'section':"Docker and System Docker",'content':"Images prefix #  When you have built your own docker registries, and have cached the burmilla/os and other os-services images, something like a normal docker pull burmilla/os can be cached as docker pull dockerhub.mycompanyname.com/docker.io/burmilla/os.\nHowever, you need a way to inject a prefix into BurmillaOS for installation or service pulls. BurmillaOS supports a global prefix you can add to force ROS to always use your mirror.\nYou can config a global image prefix:\nros config set rancher.environment.REGISTRY_DOMAIN xxxx.yyy Then you check the os list:\n$ ros os list xxxx.yyy/burmilla/os:v1.3.0 remote latest running xxxx.yyy/burmilla/os:v1.2.0 remote available ... ... Also you can check consoles:\n$ ros console switch ubuntu Switching consoles will 1. destroy the current console container 2. log you out 3. restart Docker Continue [y/N]: y Pulling console (xxxx.yyy/burmilla/os-ubuntuconsole:v1.3.0)... ... If you want to reset this setting:\nros config set rancher.environment.REGISTRY_DOMAIN docker.io "});index.add({'id':8,'href':'/docs/configuration/kernel/adding-kernel-parameters/','title':"Kernel boot parameters",'section':"Kernel",'content':"Kernel boot parameters #  BurmillaOS parses the Linux kernel boot cmdline to add any keys it understands to its configuration. This allows you to modify what cloud-init sources it will use on boot, to enable rancher.debug logging, or to almost any other configuration setting.\nThere are two ways to set or modify persistent kernel parameters, in-place (editing the file and reboot) or during installation to disk.\nIn-place editing #  Available as of RancherOS v1.1\nTo edit the kernel boot parameters of an already installed BurmillaOS system, use the new sudo ros config syslinux editing command (uses vi).\n To activate this setting, you will need to reboot.\n During installation #  If you want to set the extra kernel parameters when you are Installing BurmillaOS to Disk please use the --append parameter.\n$ sudo ros install -d /dev/sda --append \u0026#34;rancher.autologin=tty1\u0026#34; Graphical boot screen #  Available as of RancherOS v1.1\nRancherOS v1.1 added a Syslinux boot menu, which allows you to temporarily edit the boot parameters, or to select \u0026ldquo;Debug logging\u0026rdquo;, \u0026ldquo;Autologin\u0026rdquo;, both \u0026ldquo;Debug logging \u0026amp; Autologin\u0026rdquo; and \u0026ldquo;Recovery Console\u0026rdquo;.\nOn desktop systems the Syslinux boot menu can be switched to graphical mode by adding UI vesamenu.c32 to a new line in global.cfg (use sudo ros config syslinux to edit the file).\nUseful BurmillaOS kernel boot parameters #  User password #  rancher.password=\u0026lt;passwd...\u0026gt; will set the password for the rancher user. If you are not willing to use SSH keys, you can consider this parameter.\nRecovery console #  rancher.recovery=true will start a single user root bash session as easily in the boot process, with no network, or persistent filesystem mounted. This can be used to fix disk problems, or to debug your system.\nEnable/Disable sshd #  rancher.ssh.daemon=false (its enabled in the os-config) can be used to start your BurmillaOS with no sshd daemon. This can be used to further reduce the ports that your system is listening on.\nEnable debug logging #  rancher.debug=true will log everything to the console for debugging.\nAutologin console #  rancher.autologin=\u0026lt;tty...\u0026gt; will automatically log in the specified console - common values are tty1, ttyS0 and ttyAMA0 - depending on your platform.\nEnable/Disable hypervisor service auto-enable #  Available as of RancherOS v1.3\nRancherOS v1.3 added detection of Hypervisor, and then will try to download the a service called \u0026lt;hypervisor\u0026gt;-vm-tools. This may cause boot speed issues, and so can be disabled by setting rancher.hypervisor_service=false.\nAuto reboot after a kernel panic #  Available as of RancherOS v1.3\npanic=10 will automatically reboot after a kernel panic, 10 means wait 10 seconds before reboot. This is a common kernel parameter, pointing out that it is because we set this parameter by default.\n"});index.add({'id':9,'href':'/docs/configuration/kernel/loading-kernel-modules/','title':"Loading Kernel Modules",'section':"Kernel",'content':"Loading Kernel Modules #  Since BurmillaOS v0.8, we build our own kernels using an unmodified kernel.org LTS kernel. We provide both loading kernel modules with parameters and loading extra kernel modules for you.\nLoading Kernel Modules with parameters #  The rancher.modules can help you to set kernel modules or module parameters.\nAs an example, I\u0026rsquo;m going to set a parameter for kernel module ndb\n$ sudo ros config set rancher.modules \u0026#34;[\u0026#39;nbd nbds_max=1024\u0026#39;, \u0026#39;nfs\u0026#39;]\u0026#34; Or\n#cloud-config rancher: modules: [nbd nbds_max=1024, nfs] After rebooting, you can check that ndbs_max parameter has been updated.\n$ sudo cat /sys/module/nbd/parameters/nbds_max 1024 Loading Extra Kernel Modules #  We also build almost all optional extras as modules - so most in-tree modules are available in the kernel-extras service.\nIf you do need to build kernel modules for BurmillaOS, there are 4 options:\n Try the kernel-extras service Ask us to add it into the next release If its out of tree, copy the methods used for the zfs and open-iscsi services Build it yourself.  Try the kernel-extras service #  We build the BurmillaOS kernel with most of the optional drivers as kernel modules, packaged into an optional BurmillaOS service.\nTo install these, run:\n$ sudo ros service enable kernel-extras $ sudo ros service up kernel-extras The modules should now be available for you to modprobe\nAsk us to do it #  Open a GitHub issue in the https://github.com/burmilla/os repository - we\u0026rsquo;ll probably add it to the kernel-extras next time we build a kernel. Tell us if you need the module at initial configuration or boot, and we can add it to the default kernel modules.\nCopy the out of tree build method #  See https://github.com/burmilla/os-services/blob/master/z/zfs.yml and https://github.com/burmilla/os-services/tree/master/images/20-zfs\nThe build container and build.sh script build the source, and then create a tools image, which is used to \u0026ldquo;wonka.sh\u0026rdquo; import those tools into the console container using docker run\nBuild your own. #  As an example I\u0026rsquo;m going build the intel-ishtp hid driver using the burmilla/os-zfs:\u0026lt;version\u0026gt; images to build in, as they should contain the right tools versions for that kernel.\nsudo docker run --rm -it --entrypoint bash --privileged -v /lib:/host/lib -v $(pwd):/data -w /data burmilla/os-zfs:$(ros -v | cut -d \u0026#39; \u0026#39; -f 2) apt-get update apt-get install -qy libncurses5-dev bc libssh-dev curl -SsL -o src.tgz https://github.com/burmilla/os-kernel/releases/download/v$(uname -r)/linux-$(uname -r)-src.tgz tar zxvf src.tgz zcat /proc/config.gz \u0026gt;.config # Yes, ignore the name of the directory :/ cd v* # enable whatever modules you want to add. make menuconfig # I finally found an Intel sound hub that wasn\u0026#39;t enabled yet # CONFIG_INTEL_ISH_HID=m make modules SUBDIRS=drivers/hid/intel-ish-hid # test it insmod drivers/hid/intel-ish-hid/intel-ishtp.ko rmmod intel-ishtp # install it ln -s /host/lib/modules/ /lib/ cp drivers/hid/intel-ish-hid/*.ko /host/lib/modules/$(uname -r)/kernel/drivers/hid/ depmod # done exit Then in your console, you should be able to run\nmodprobe intel-ishtp "});index.add({'id':10,'href':'/docs/configuration/docker/private-registries/','title':"Private Registries",'section':"Docker and System Docker",'content':"Private Registries #  When launching services through a cloud-config, it is sometimes necessary to pull a private image from DockerHub or from a private registry. Authentication for these can be embedded in your cloud-config.\nFor example, to add authentication for DockerHub:\n#cloud-config rancher: registry_auths: https://index.docker.io/v1/: auth: dXNlcm5hbWU6cGFzc3dvcmQ= The auth key is generated by base64 encoding a string of the form username:password. The docker login command can be used to generate an auth key. After running the command and authenticating successfully, the key can be found in the $HOME/.docker/config.json file.\n{ \u0026#34;auths\u0026#34;: { \u0026#34;https://index.docker.io/v1/\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;dXNlcm5hbWU6cGFzc3dvcmQ=\u0026#34; } } } Alternatively, a username and password can be specified directly.\n#cloud-config rancher: registry_auths: https://index.docker.io/v1/: username: username password: password Docker Client Authentication #  Configuring authentication for the Docker client is not handled by the registry_auth key. Instead, the write_files directive can be used to write credentials to the standard Docker configuration location.\n#cloud-config write_files: - path: /home/rancher/.docker/config.json permissions: \u0026#34;0755\u0026#34; owner: burmilla content: |{ \u0026#34;auths\u0026#34;: { \u0026#34;https://index.docker.io/v1/\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;asdf=\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;not@val.id\u0026#34; } } } Certificates for Private Registries #  Certificates can be stored in the standard locations (i.e. /etc/docker/certs.d) following the Docker documentation. By using the write_files directive of the cloud-config, the certificates can be written directly into /etc/docker/certs.d.\n#cloud-config write_files: - path: /etc/docker/certs.d/myregistrydomain.com:5000/ca.crt permissions: \u0026#34;0644\u0026#34; owner: root content: |-----BEGIN CERTIFICATE----- MIIDJjCCAg4CCQDLCSjwGXM72TANBgkqhkiG9w0BAQUFADBVMQswCQYDVQQGEwJB VTETMBEGA1UECBMKU29tZS1TdGF0ZTEhMB8GA1UEChMYSW50ZXJuZXQgV2lkZ2l0 cyBQdHkgTHRkMQ4wDAYDVQQDEwVhbGVuYTAeFw0xNTA3MjMwMzUzMDdaFw0xNjA3 MjIwMzUzMDdaMFUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0YXRlMSEw HwYDVQQKExhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQxDjAMBgNVBAMTBWFsZW5h MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxdVIDGlAySQmighbfNqb TtqetENPXjNNq1JasIjGGZdOsmFvNciroNBgCps/HPJphICQwtHpNeKv4+ZuL0Yg 1FECgW7oo6DOET74swUywtq/2IOeik+i+7skmpu1o9uNC+Fo+twpgHnGAaGk8IFm fP5gDgthrWBWlEPTPY1tmPjI2Hepu2hJ28SzdXi1CpjfFYOiWL8cUlvFBdyNqzqT uo6M2QCgSX3E1kXLnipRT6jUh0HokhFK4htAQ3hTBmzcxRkgTVZ/D0hA5lAocMKX EVP1Tlw0y1ext2ppS1NR9Sg46GP4+ATgT1m3ae7rWjQGuBEB6DyDgyxdEAvmAEH4 LQIDAQABMA0GCSqGSIb3DQEBBQUAA4IBAQA45V0bnGPhIIkb54Gzjt9jyPJxPVTW mwTCP+0jtfLxAor5tFuCERVs8+cLw1wASfu4vH/yHJ/N/CW92yYmtqoGLuTsywJt u1+amECJaLyq0pZ5EjHqLjeys9yW728IifDxbQDX0cj7bBjYYzzUXp0DB/dtWb/U KdBmT1zYeKWmSxkXDFFSpL/SGKoqx3YLTdcIbgNHwKNMfTgD+wTZ/fvk0CLxye4P n/1ZWdSeZPAgjkha5MTUw3o1hjo/0H0ekI4erZFrZnG2N3lDaqDPR8djR+x7Gv6E vloANkUoc1pvzvxKoz2HIHUKf+xFT50xppx6wsQZ01pNMSNF0qgc1vvH -----END CERTIFICATE----- "});index.add({'id':11,'href':'/docs/configuration/advanced/resizing-device-partition/','title':"Resizing a Device Partition",'section':"Advanced",'content':"Resizing a Device Partition #  The resize_device cloud config option can be used to automatically extend the first partition (assuming its ext4) to fill the size of it\u0026rsquo;s device.\nOnce the partition has been resized to fill the device, a /var/lib/rancher/resizefs.done file will be written to prevent the resize tools from being run again. If you need it to run again, delete that file and reboot.\n#cloud-config rancher: resize_device: /dev/sda This behavior is the default when launching BurmillaOS on AWS.\n"});index.add({'id':12,'href':'/docs/configuration/advanced/running-commands/','title':"Running Commands",'section':"Advanced",'content':"Running Commands #  You can automate running commands on boot using the runcmd cloud-config directive. Commands can be specified as either a list or a string. In the latter case, the command is executed with sh.\n#cloud-config runcmd: - [ touch, /home/rancher/test1 ] - echo \u0026#34;test\u0026#34; \u0026gt; /home/rancher/test2 Commands specified using runcmd will be executed within the context of the console container.\nRunning Docker commands #  When using runcmd, BurmillaOS will wait for all commands to complete before starting Docker. As a result, any docker run command should not be placed under runcmd. Instead, the /etc/rc.local script can be used. BurmillaOS will not wait for commands in this script to complete, so you can use the wait-for-docker command to ensure that the Docker daemon is running before performing any docker run commands.\n#cloud-config rancher: write_files: - path: /etc/rc.local permissions: \u0026#34;0755\u0026#34; owner: root content: |#!/bin/bash wait-for-docker docker run -d nginx Running Docker commands in this manner is useful when pieces of the docker run command are dynamically generated. For services whose configuration is static, adding a system service is recommended.\n"});index.add({'id':13,'href':'/docs/configuration/base/hostname/','title':"Setting the Hostname",'section':"Base",'content':"Setting the Hostname #  You can set the hostname of the host using cloud-config. The example below shows how to configure it.\n#cloud-config hostname: myhost "});index.add({'id':14,'href':'/docs/configuration/docker/setting-up-docker-tls/','title':"Setting up Docker TLS",'section':"Docker and System Docker",'content':"Setting up Docker TLS #  ros tls generate is used to generate both the client and server TLS certificates for Docker.\nRemember, all ros commands need to be used with sudo or as a root user.\nEnd to end example #  Enable TLS for Docker and Generate Server Certificate #  To have docker secured by TLS you need to set rancher.docker.tls to true, and generate a set of server and client keys and certificates:\n$ sudo ros config set rancher.docker.tls true $ sudo ros tls gen --server -H localhost -H \u0026lt;hostname1\u0026gt; -H \u0026lt;hostname2\u0026gt; ... -H \u0026lt;hostnameN\u0026gt; $ sudo system-docker restart docker Here, \u0026lt;hostname*\u0026gt;s are the hostnames that you will be able to use as your docker host names. A \u0026lt;hostname*\u0026gt; can be a wildcard pattern, e.g. \u0026ldquo;*.*.*.*.*\u0026rdquo;. It is recommended to have localhost as one of the hostnames, so that you can test docker TLS connectivity locally.\nWhen you\u0026rsquo;ve done that, all the necessary server certificate and key files have been saved to /etc/docker/tls directory, and the docker service has been started with --tlsverify option.\nGenerate Client Certificates #  You also need client cert and key to access Docker via a TCP socket now:\n$ sudo ros tls gen INFO[0000] Out directory (-d, --dir) not specified, using default: /home/rancher/.docker All the docker client TLS files are in ~/.docker dir now.\nTest docker TLS connection #  Now you can use your client cert to check if you can access Docker via TCP:\n$ docker --tlsverify version Because all the necessary files are in the ~/.docker dir, you don\u0026rsquo;t need to specify them using --tlscacert --tlscert and --tlskey options. You also don\u0026rsquo;t need -H to access Docker on localhost.\nCopy the files from /home/rancher/.docker to $HOME/.docker on your client machine if you need to access Docker on your BurmillaOS host from there.\nOn your client machine, set the Docker host and test out if Docker commands work.\n$ export DOCKER_HOST=tcp://\u0026lt;hostname\u0026gt;:2376 DOCKER_TLS_VERIFY=1 $ docker ps "});index.add({'id':15,'href':'/docs/configuration/base/ssh-keys/','title':"SSH Settings",'section':"Base",'content':"SSH Settings #  BurmillaOS supports adding SSH keys through the cloud-config file. Within the cloud-config file, you simply add the ssh keys within the ssh_authorized_keys key.\n#cloud-config ssh_authorized_keys: - ssh-rsa AAA...ZZZ example1@burmilla - ssh-rsa BBB...ZZZ example2@burmilla When we pass the cloud-config file during the ros install command, it will allow these ssh keys to be associated with the rancher user. You can ssh into BurmillaOS using the key.\n$ ssh -i /path/to/private/key rancher@\u0026lt;ip-address\u0026gt; Please note that OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use.\nSSHD Port and IP #  BurmillaOS supports changing the sshd port and IP, you can use these in the cloud-config file:\nrancher: ssh: port: 10022 listen_address: 172.22.100.100 These settings are only designed for default console. Because if you change sshd-config, restart the host will restore the default, the new configuration will not take effect.\nFor other consoles, all files are persistent, you can modify sshd-config by yourself.\n"});index.add({'id':16,'href':'/docs/configuration/docker/switching-docker-versions/','title':"Switching Docker Versions",'section':"Docker and System Docker",'content':"Switching Docker Versions #  The version of User Docker used in BurmillaOS can be configured using a cloud-config file or by using the ros engine command.\n Note: There are known issues in Docker when switching between versions. For production systems, we recommend setting the Docker engine only once using a cloud-config.\n Available Docker engines #  The ros engine list command can be used to show which Docker engines are available to switch to. This command will also provide details of which Docker engine is currently being used.\n$ sudo ros engine list --update current docker-19.03.13 disabled docker-19.03.14 disabled docker-20.10.0 Setting the Docker engine using cloud-config #  BurmillaOS supports defining which Docker engine to use through the cloud-config file. To change the Docker version from the default packaged version, you can use the following cloud-config setting and select one of the available engines. In the following example, we\u0026rsquo;ll use the cloud-config file to set BurmillaOS to use Docker 1.10.3 for User Docker.\n#cloud-config rancher: docker: engine: docker-19.03.13 Changing Docker engines after BurmillaOS has started #  If you\u0026rsquo;ve already started BurmillaOS and want to switch Docker engines, you can change the Docker engine by using the ros engine switch command. In our example, we\u0026rsquo;ll switch to Docker 19.03.13.\n$ sudo ros engine switch docker-19.03.13 INFO[0001] Project [os]: Starting project INFO[0003] [0/18] [docker]: Starting INFO[0003] Recreating docker INFO[0003] [1/18] [docker]: Started INFO[0003] Project [os]: Project started $ docker version Client: Docker Engine - Community Version: 19.03.13 API version: 1.40 Go version: go1.13.15 Git commit: 4484c46 Built: Wed Sep 16 16:58:04 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.13 API version: 1.40 (minimum version 1.12) Go version: go1.13.15 Git commit: 4484c46 Built: Wed Sep 16 17:04:43 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.3.7 GitCommit: 8fba4e9a7d01810a393d5d25a3621dc101981175 runc: Version: 1.0.0-rc10 GitCommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init: Version: 0.18.0 GitCommit: fec3683 Enabling Docker engines #  If you don\u0026rsquo;t want to automatically switch Docker engines, you can also set which version of Docker to use after the next reboot by enabling a Docker engine.\n$ sudo ros engine enable docker-19.03.13 Using a Custom Version of Docker #  If you\u0026rsquo;re using a version of Docker that isn\u0026rsquo;t available by default or a custom build of Docker then you can create a custom Docker image and service file to distribute it.\nDocker engine images are built by adding the binaries to a folder named engine and then adding this folder to a FROM scratch image. For example, the following Dockerfile will build a Docker engine image.\nFROMscratchCOPY engine /engineOnce the image is built a system service configuration file must be created. An example file can be found in the burmilla/os-services repo. Change the image field to point to the Docker engine image you\u0026rsquo;ve built.\nAll of the previously mentioned methods of switching Docker engines are now available. For example, if your service file is located at https://myservicefile then the following cloud-config file could be used to use your custom Docker engine.\n#cloud-config rancher: docker: engine: https://myservicefile "});index.add({'id':17,'href':'/docs/configuration/advanced/sysctl/','title':"Sysctl Settings",'section':"Advanced",'content':"Sysctl Settings #  The rancher.sysctl cloud-config key can be used to control sysctl parameters. This works in a manner similar to /etc/sysctl.conf for other Linux distros.\n#cloud-config rancher: sysctl: net.ipv4.conf.default.rp_filter: 1 You can either add these settings to your cloud-init.yml, or use sudo ros config merge -i somefile.yml to merge settings into your existing system.\n"});index.add({'id':18,'href':'/docs/configuration/base/users/','title':"Users",'section':"Base",'content':"Users #  Currently, we don\u0026rsquo;t support adding other users besides rancher.\nYou can add users in the console container, but these users will only exist as long as the console container exists. It only makes sense to add users in a persistent consoles.\nIf you want the console user to be able to ssh into BurmillaOS, you need to add them to the docker group.\n"});index.add({'id':19,'href':'/docs/configuration/advanced/write-files/','title':"Writing Files",'section':"Advanced",'content':"Writing Files #  You can automate writing files to disk using the write_files cloud-config directive.\n#cloud-config write_files: - path: /etc/rc.local permissions: \u0026#34;0755\u0026#34; owner: root content: |#!/bin/bash echo \u0026#34;I\u0026#39;m doing things on start\u0026#34; Writing Files in Specific System Services #  By default, the write_files directive will create files in the console container. To write files in other system services, the container key can be used. For example, the container key could be used to write to /etc/ntp.conf in the NTP system service.\n#cloud-config write_files: - container: ntp path: /etc/ntp.conf permissions: \u0026#34;0644\u0026#34; owner: root content: |server 0.pool.ntp.org iburst server 1.pool.ntp.org iburst server 2.pool.ntp.org iburst server 3.pool.ntp.org iburst # Allow only time queries, at a limited rate, sending KoD when in excess. # Allow all local queries (IPv4, IPv6) restrict default nomodify nopeer noquery limited kod restrict 127.0.0.1 restrict [::1]  Note: Currently, writing files to a specific system service is only supported for BurmillaOS\u0026rsquo;s built-in services. You are unable to write files to any custom system services.\n "});index.add({'id':20,'href':'/docs/configuration/advanced/microcode-loader/','title':"Microcode Loader",'section':"Advanced",'content':"How to update microcode #  Processor manufacturers release stability and security updates to the processor microcode. While microcode can be updated through the BIOS, the Linux kernel is also able to apply these updates. These updates provide bug fixes that can be critical to the stability of your system. Without these updates, you may experience spurious crashes or unexpected system halts that can be difficult to track down.\nThe microcode loader supports three loading methods:\n Early load microcode Late loading Builtin microcode  You can get more details from here.\nBurmillaOS supports Late loading. To update the Intel microcode, get the latest Intel microcode. An example is here. Then copy the data files to the firmware directory:\nmkdir -p /lib/firmware/intel-ucode/ cp -v intel-ucode/* /lib/firmware/intel-ucode/ Reload the microcode. This file does not exist if you are running BurmillaOS on the hypervisor. Usually, the VM does not need to update the microcode.\necho 1 \u0026gt; /sys/devices/system/cpu/microcode/reload Check the result:\ndmesg | grep microcode [ 13.659429] microcode: sig=0x306f2, pf=0x1, revision=0x36 [ 13.665981] microcode: Microcode Update Driver: v2.01 \u0026lt;tigran@aivazian.fsnet.co.uk\u0026gt;, Peter Oruba [ 510.899733] microcode: updated to revision 0x3b, date = 2017-11-17 You can use runcmd to reload the microcode every boot:\nruncmd: - echo 1 \u0026gt; /sys/devices/system/cpu/microcode/reload "});index.add({'id':21,'href':'/docs/installation/','title':"Installation",'section':"Docs",'content':"Installing and Running BurmillaOS #  BurmillaOS runs on virtualization platforms, cloud providers and bare metal servers. We also support running a local VM on your laptop.\nTo start running BurmillaOS as quickly as possible, follow our Quick Start Guide.\nPlatforms #  Refer to the below resources for more information on installing BurmillaOS on your platform.\nWorkstation #    Docker Machine  Boot from ISO  Apple Silicon  Cloud #    Amazon EC2  Google Compute Engine  DigitalOcean  Azure  OpenStack  VMware ESXi  Aliyun  Bare Metal \u0026amp; Virtual Servers #    PXE  Install to Hard Disk  Raspberry Pi  "});index.add({'id':22,'href':'/docs/installation/upgrading/','title':"Upgrading",'section':"Installation",'content':"Upgrading #  If BurmillaOS has released a new version and you want to learn how to upgrade your OS, we make it easy using the ros os command.\nSince BurmillaOS is a kernel and initrd, the upgrade process is downloading a new kernel and initrd, and updating the boot loader to point to it. The old kernel and initrd are not removed. If there is a problem with your upgrade, you can select the old kernel from the Syslinux bootloader.\nBefore upgrading to any version, please review the release notes on our releases page in GitHub to review any updates in the release.\n Note: If you are using docker-machine then you will not be able to upgrade your BurmillaOS version. You need to delete and re-create the machine.\n Upgrade RancherOS to BurmillaOS #  Permanently upgrade your existing RancherOS installation to BurmillaOS and begin tracking BurmillaOS releases:\n$ sudo ros config set rancher.upgrade.url \\ https://raw.githubusercontent.com/burmilla/releases/v2.0.x/releases.yml $ sudo ros os upgrade After reboot run also:\n$ sudo ros console switch default other why you are still using console from RancherOS (visible in sudo system-docker ps output)\nVersion Control #  First, let\u0026rsquo;s check what version you have running on your system.\n$ sudo ros os version v2.0.0 If you just want to find out the available releases from the command line, it\u0026rsquo;s a simple command.\n# List all available releases $ sudo ros os list burmilla/os:v2.0.0 local latest running burmilla/os:v2.0.0 remote available The local/remote label shows which images are available to System Docker locally versus which need to be pulled from Docker Hub. If you choose to upgrade to a version that is remote, we will automatically pull that image during the upgrade.\nUpgrading #  Let\u0026rsquo;s walk through upgrading! The ros os upgrade command will automatically upgrade to the current release of BurmillaOS. The current release is designated as the most recent release of BurmillaOS.\n$ sudo ros os upgrade Upgrading to burmilla/os:v2.0.0 Confirm that you want to continue and the final step will be to confirm that you want to reboot.\nContinue [y/N]: y ... ... ... Continue with reboot [y/N]: y INFO[0037] Rebooting  Note: The default console container is persistent and will NOT be updated by default. Use ros console switch default to update the os-console container. This is a destructive operation - see console persistence for info about what will be retained.\n  Note: To make the default container non-persistent, set io.docker.compose.rebuild: \u0026quot;always\u0026quot; on the console container os-config.tpl\n After rebooting, you can check that your version has been updated.\n$ sudo ros -v version v2.0.0 from os image burmilla/os:v2.0.0  Note: If you are booting from ISO and have not installed to disk, your upgrade will not be saved. You can view our guide to installing to disk.\n Upgrading to a Specific Version #  If you are a couple of versions behind the current version, use the -i option to pick the version that you want to upgrade to.\n$ sudo ros os upgrade -i burmilla/os:v2.0.0 Upgrading to burmilla/os:v2.0.0 Continue [y/N]: y ... ... ... Continue with reboot [y/N]: y INFO[0082] Rebooting Bypassing The Prompts #  We have added the ability to bypass the prompts. Use the -f or --force option when upgrading. Your machine will automatically be rebooted and you\u0026rsquo;ll just need to log back in when it\u0026rsquo;s done.\nIf you want to bypass the prompts, but you don\u0026rsquo;t want to immediately reboot, you can add --no-reboot to avoid rebooting immediately.\nRolling back an Upgrade #  If you\u0026rsquo;ve upgraded your BurmillaOS and something\u0026rsquo;s not working anymore, you can easily rollback your upgrade.\nThe ros os upgrade command works for rolling back. We\u0026rsquo;ll use the -i option to \u0026ldquo;upgrade\u0026rdquo; to a specific version. All you need to do is pick the previous version! Same as before, you will be prompted to confirm your upgrade version as well as confirm your reboot.\n$ sudo ros -v ros version v1.9.0 $ sudo ros os upgrade -i burmilla/os:v2.0.0 Upgrading to burmilla/os:v2.0.0 Continue [y/N]: y ... ... ... Continue with reboot [y/N]: y INFO[0082] Rebooting After rebooting, the rollback will be complete.\n$ sudo ros -v version v2.0.0 from os image burmilla/os:v2.0.0  Note: If you are using a persistent console and in the current version\u0026rsquo;s console, rolling back is not supported. For example, rolling back to v0.4.5 when using a v0.5.0 persistent console is not supported.\n Staging an Upgrade #  During an upgrade, the template of the upgrade is downloaded from the burmilla/os repository. You can download this template ahead of time so that it\u0026rsquo;s saved locally. This will decrease the time it takes to upgrade. We\u0026rsquo;ll use the -s option to stage the specific template. You will need to specify the image name with the -i option, otherwise it will automatically stage the current version.\n$ sudo ros os upgrade -s -i burmilla/os:v2.0.0 Custom Upgrade Sources #  In the upgrade key, the url is used to find the list of available and current versions of BurmillaOS. This can be modified to track custom builds and releases.\n#cloud-config rancher: upgrade: url: https://raw.githubusercontent.com/burmilla/releases/master/releases.yml image: burmilla/os "});index.add({'id':23,'href':'/docs/configuration/advanced/','title':"Advanced",'section':"Configuration",'content':"Advanced Configuration Options #    Air Gap Configuration  Disabling Access to BurmillaOS  How to update microcode  Resizing a Device Partition  Running Commands  Sysctl Settings  Writing Files  "});index.add({'id':25,'href':'/docs/configuration/','title':"Configuration",'section':"Docs",'content':"Configuration #  There are two ways that BurmillaOS can be configured.\n A cloud-config file can be used to provide configuration when first booting BurmillaOS. Manually changing configuration with the ros config command.  Typically, when you first boot the server, you pass in a cloud-config file to configure the initialization of the server. After the first boot, if you have any changes for the configuration, it\u0026rsquo;s recommended that you use ros config to set the necessary configuration properties. Any changes will be saved on disk and a reboot will be required for changes to be applied.\nCloud-Config #  Cloud-config is a declarative configuration file format supported by many Linux distributions and is the primary configuration mechanism for BurmillaOS.\nA Linux OS supporting cloud-config will invoke a cloud-init process during startup to parse the cloud-config file and configure the operating system. BurmillaOS runs its own cloud-init process in a system container. The cloud-init process will attempt to retrieve a cloud-config file from a variety of data sources. Once cloud-init obtains a cloud-config file, it configures the Linux OS according to the content of the cloud-config file.\nWhen you create a BurmillaOS instance on AWS, for example, you can optionally provide cloud-config passed in the user-data field. Inside the BurmillaOS instance, cloud-init process will retrieve the cloud-config content through its AWS cloud-config data source, which simply extracts the content of user-data received by the VM instance. If the file starts with \u0026ldquo;#cloud-config\u0026rdquo;, cloud-init will interpret that file as a cloud-config file. If the file starts with #!\u0026lt;interpreter\u0026gt; (e.g., #!/bin/sh), cloud-init will simply execute that file. You can place any configuration commands in the file as scripts.\nA cloud-config file uses the YAML format. YAML is easy to understand and easy to parse. For more information on YAML, please read more at the YAML site. The most important formatting principle is indentation or whitespace. This indentation indicates relationships of the items to one another. If something is indented more than the previous line, it is a sub-item of the top item that is less indented.\nExample: Notice how both are indented underneath ssh_authorized_keys.\n#cloud-config ssh_authorized_keys: - ssh-rsa AAA...ZZZ example1@burmilla - ssh-rsa BBB...ZZZ example2@burmilla In our example above, we have our #cloud-config line to indicate it\u0026rsquo;s a cloud-config file. We have 1 top-level property, ssh_authorized_keys. Its value is a list of public keys that are represented as a dashed list under ssh_authorized_keys:.\nManually Changing Configuration #  To update BurmillaOS configuration after booting, the ros config set \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; command can be used. For more complicated settings, like the sysctl settings, you can also create a small YAML file and then run sudo ros config merge -i \u0026lt;your yaml file\u0026gt;.\nGetting Values #  You can easily get any value that\u0026rsquo;s been set in the /var/lib/rancher/conf/cloud-config.yml file. Let\u0026rsquo;s see how easy it is to get the DNS configuration of the system.\n$ sudo ros config get rancher.network.dns.nameservers - 8.8.8.8 - 8.8.4.4 Setting Values #  You can set values in the /var/lib/rancher/conf/cloud-config.yml file.\nSetting a simple value in the /var/lib/rancher/conf/cloud-config.yml\n$ sudo ros config set rancher.docker.tls true Setting a list in the /var/lib/rancher/conf/cloud-config.yml\n$ sudo ros config set rancher.network.dns.nameservers \u0026#34;[\u0026#39;8.8.8.8\u0026#39;,\u0026#39;8.8.4.4\u0026#39;]\u0026#34; Exporting the Current Configuration #  To output and review the current configuration state you can use the ros config export command.\n$ sudo ros config export rancher: docker: tls: true network: dns: nameservers: - 8.8.8.8 - 8.8.4.4 Validating a Configuration File #  To validate a configuration file you can use the ros config validate command.\n$ sudo ros config validate -i cloud-config.yml "});index.add({'id':26,'href':'/docs/configuration/base/','title':"Base",'section':"Configuration",'content':"Configuration #  There are two ways that BurmillaOS can be configured.\n A cloud-config file can be used to provide configuration when first booting BurmillaOS. Manually changing configuration with the ros config command.  Typically, when you first boot the server, you pass in a cloud-config file to configure the initialization of the server. After the first boot, if you have any changes for the configuration, it\u0026rsquo;s recommended that you use ros config to set the necessary configuration properties. Any changes will be saved on disk and a reboot will be required for changes to be applied.\nCloud-Config #  Cloud-config is a declarative configuration file format supported by many Linux distributions and is the primary configuration mechanism for BurmillaOS.\nA Linux OS supporting cloud-config will invoke a cloud-init process during startup to parse the cloud-config file and configure the operating system. BurmillaOS runs its own cloud-init process in a system container. The cloud-init process will attempt to retrieve a cloud-config file from a variety of data sources. Once cloud-init obtains a cloud-config file, it configures the Linux OS according to the content of the cloud-config file.\nWhen you create a BurmillaOS instance on AWS, for example, you can optionally provide cloud-config passed in the user-data field. Inside the BurmillaOS instance, cloud-init process will retrieve the cloud-config content through its AWS cloud-config data source, which simply extracts the content of user-data received by the VM instance. If the file starts with \u0026ldquo;#cloud-config\u0026rdquo;, cloud-init will interpret that file as a cloud-config file. If the file starts with #!\u0026lt;interpreter\u0026gt; (e.g., #!/bin/sh), cloud-init will simply execute that file. You can place any configuration commands in the file as scripts.\nA cloud-config file uses the YAML format. YAML is easy to understand and easy to parse. For more information on YAML, please read more at the YAML site. The most important formatting principle is indentation or whitespace. This indentation indicates relationships of the items to one another. If something is indented more than the previous line, it is a sub-item of the top item that is less indented.\nExample: Notice how both are indented underneath ssh_authorized_keys.\n#cloud-config ssh_authorized_keys: - ssh-rsa AAA...ZZZ example1@burmilla - ssh-rsa BBB...ZZZ example2@burmilla In our example above, we have our #cloud-config line to indicate it\u0026rsquo;s a cloud-config file. We have 1 top-level property, ssh_authorized_keys. Its value is a list of public keys that are represented as a dashed list under ssh_authorized_keys:.\nManually Changing Configuration #  To update BurmillaOS configuration after booting, the ros config set \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; command can be used. For more complicated settings, like the sysctl settings, you can also create a small YAML file and then run sudo ros config merge -i \u0026lt;your yaml file\u0026gt;.\nGetting Values #  You can easily get any value that\u0026rsquo;s been set in the /var/lib/rancher/conf/cloud-config.yml file. Let\u0026rsquo;s see how easy it is to get the DNS configuration of the system.\n$ sudo ros config get rancher.network.dns.nameservers - 8.8.8.8 - 8.8.4.4 Setting Values #  You can set values in the /var/lib/rancher/conf/cloud-config.yml file.\nSetting a simple value in the /var/lib/rancher/conf/cloud-config.yml\n$ sudo ros config set rancher.docker.tls true Setting a list in the /var/lib/rancher/conf/cloud-config.yml\n$ sudo ros config set rancher.network.dns.nameservers \u0026#34;[\u0026#39;8.8.8.8\u0026#39;,\u0026#39;8.8.4.4\u0026#39;]\u0026#34; Exporting the Current Configuration #  To output and review the current configuration state you can use the ros config export command.\n$ sudo ros config export rancher: docker: tls: true network: dns: nameservers: - 8.8.8.8 - 8.8.4.4 Validating a Configuration File #  To validate a configuration file you can use the ros config validate command.\n$ sudo ros config validate -i cloud-config.yml "});index.add({'id':27,'href':'/docs/networking/','title':"Networking",'section':"Docs",'content':"Configuring Network Interfaces #  Using ros config, you can configure specific interfaces. Wildcard globbing is supported so eth* will match eth1 and eth2. The available options you can configure are address, gateway, mtu, and dhcp.\n$ sudo ros config set rancher.network.interfaces.eth1.address 172.68.1.100/24 $ sudo ros config set rancher.network.interfaces.eth1.gateway 172.68.1.1 $ sudo ros config set rancher.network.interfaces.eth1.mtu 1500 $ sudo ros config set rancher.network.interfaces.eth1.dhcp false If you wanted to configure the interfaces through the cloud config file, you\u0026rsquo;ll need to place interface configurations within the rancher key.\n#cloud-config rancher: network: interfaces: eth1: address: 172.68.1.100/24 gateway: 172.68.1.1 mtu: 1500 dhcp: false  Note: The address item should be the CIDR format.\n Multiple NICs #  If you want to configure one of multiple network interfaces, you can specify the MAC address of the interface you want to configure.\nUsing ros config, you can specify the MAC address of the NIC you want to configure as follows:\n$ sudo ros config set rancher.network.interfaces.”mac=ea:34:71:66:90:12:01”.dhcp true Alternatively, you can place the MAC address selection in your cloud config file as follows:\n#cloud-config rancher: network: interfaces: \u0026#34;mac=ea:34:71:66:90:12:01\u0026#34;: dhcp: true NIC bonding #  You can aggregate several network links into one virtual link for redundancy and increased throughput. For example:\n#cloud-config rancher: network: interfaces: bond0: addresses: - 192.168.101.33/31 - 10.88.23.129/31 gateway: 192.168.101.32 bond_opts: downdelay: \u0026#34;200\u0026#34; lacp_rate: \u0026#34;1\u0026#34; miimon: \u0026#34;100\u0026#34; mode: \u0026#34;4\u0026#34; updelay: \u0026#34;200\u0026#34; xmit_hash_policy: layer3+4 post_up: - ip route add 10.0.0.0/8 via 10.88.23.128 mac=0c:c4:d7:b2:14:d2: bond: bond0 mac=0c:c4:d7:b2:14:d3: bond: bond0 In this example two physical NICs (with MACs 0c:c4:d7:b2:14:d2 and 0c:c4:d7:b2:14:d3) are aggregated into a virtual one bond0.\nDuring the bootup process, BurmillaOS runs cloud-init. It automatically detects the data sources of cloud-init, but sometimes a data source requires a network connection. By default, in cloud-init, we open rancher.network.interfaces.eth*.dhcp=true, which may affect the bonding NIC. If you do not require the network connection for your data-source, use rancher.network.interfaces.eth*.dhcp=false in the kernel cmdline to disable DHCP for all NICs.\nVLANS #  In this example, you can create an interface eth0.100 which is tied to VLAN 100 and an interface foobar that will be tied to VLAN 200.\n#cloud-config rancher: network: interfaces: eth0: vlans: 100,200:foobar Bridging #  In this example, you can create a bridge interface.\n#cloud-config rancher: network: interfaces: br0: bridge: true dhcp: true eth0: bridge: br0 Run custom network configuration commands #  Available as of RancherOS v1.1\nYou can configure pre and post network configuration commands to run in the network service container by adding pre_cmds and post_cmds array keys to rancher.network, or pre_up andpost_up keys for specific rancher.network.interfaces.\nFor example:\n#cloud-config write_files: - container: network path: /var/lib/iptables/rules.sh permissions: \u0026#34;0755\u0026#34; owner: root:root content: |#!/bin/bash set -ex echo $@ \u0026gt;\u0026gt; /var/log/net.log # the last line of the file needs to be a blank line or a comment rancher: network: dns: nameservers: - 8.8.4.4 - 4.2.2.3 pre_cmds: - /var/lib/iptables/rules.sh pre_cmds post_cmds: - /var/lib/iptables/rules.sh post_cmds interfaces: lo: pre_up: - /var/lib/iptables/rules.sh pre_up lo post_up: - /var/lib/iptables/rules.sh post_up lo eth0: pre_up: - /var/lib/iptables/rules.sh pre_up eth0 post_up: - /var/lib/iptables/rules.sh post_up eth0 eth1: dhcp: true pre_up: - /var/lib/iptables/rules.sh pre_up eth1 post_up: - /var/lib/iptables/rules.sh post_up eth1 eth2: address: 192.168.3.13/16 mtu: 1450 pre_up: - /var/lib/iptables/rules.sh pre_up eth2 post_up: - /var/lib/iptables/rules.sh post_up eth2 WiFi #  Available as of RancherOS v1.5\nIn order to enable WiFi access, update the cloud-config with the WiFi network information. You can use DHCP or STATIC mode.\nExample of a wireless adapter using DHCP #  #cloud-config rancher: network: interfaces: wlan0: wifi_network: network1 wifi_networks: network1: ssid: \u0026#34;Your wifi ssid\u0026#34; psk: \u0026#34;Your wifi password\u0026#34; scan_ssid: 1 Example single adapter #  This Adapter uses a specified network to connect to and sets the IP statically:\nrancher: network: dns: nameservers: - 8.8.8.8 - 8.8.4.4 interfaces: wlan0: wifi_network: network1 wifi_networks: network1: ssid: \u0026#34;Your wifi ssid\u0026#34; psk: \u0026#34;Your wifi password\u0026#34; scan_ssid: 1 address: 192.168.1.78/24 gateway: 192.168.1.1 Example multiple adapters #  This configuration connects to multiple wireless networks and uses DHCP on each of them:\nrancher: network: interfaces: wlan0: wifi_network: network1 wlan1: wifi_network: network2 wifi_networks: network1: ssid: \u0026#34;Your wifi ssid\u0026#34; psk: \u0026#34;Your wifi password\u0026#34; scan_ssid: 1 network2: ssid: \u0026#34;Your wifi ssid\u0026#34; psk: \u0026#34;Your wifi password\u0026#34; scan_ssid: 1 When adding in WiFi access, you do not need a system reboot, you only need to restart the network service in System Docker.\n$ sudo system-docker restart network  Note: For Intel wireless adapters, there are some built-in firmware and modules, which prevents requiring to install any new modules or firmware. For other adapters, you may need to install additional os kernel-extras.\n 4G-LTE #  Available as of RancherOS v1.5\nIn order to support 4G-LTE, 4G-LTE module will need to be connected to the motherboard and to get a good signal, an external antenna will need to be added. You can assemble such a device, which supports USB interface and SIM cards slot.\nIn order to use BurmillaOS, you will need to use the ISO built for 4G-LTE support. This ISO has a built-in modem-manager service and is available with each release.\nAfter booting the ISO, there will be a 4G NIC, such as wwan0. Use the following cloud-config to set the APN parameter.\nrancher: network: modem_networks: wwan0: apn: xxx After any configuration changes, restart the modem-manager service to apply these changes.\n$ sudo system-docker restart modem-manager  Note: Currently, BurmillaOS has some built-in rules in udev rules to allow BurmillaOS to recognize specific 4G devices, but there are additional vendors that may be missing. If you need to add these in, please file an issue.\n "});index.add({'id':28,'href':'/docs/configuration/docker/','title':"Docker and System Docker",'section':"Configuration",'content':"Configuring Docker or System Docker #  In BurmillaOS, you can configure System Docker and Docker daemons by using cloud-config.\nConfiguring Docker #  In your cloud-config, Docker configuration is located under the rancher.docker key.\n#cloud-config rancher: docker: tls: true tls_args: - \u0026#34;--tlsverify\u0026#34; - \u0026#34;--tlscacert=/etc/docker/tls/ca.pem\u0026#34; - \u0026#34;--tlscert=/etc/docker/tls/server-cert.pem\u0026#34; - \u0026#34;--tlskey=/etc/docker/tls/server-key.pem\u0026#34; - \u0026#34;-H=0.0.0.0:2376\u0026#34; storage_driver: overlay You can also customize Docker after it\u0026rsquo;s been started using ros config.\n$ sudo ros config set rancher.docker.storage_driver overlay User Docker settings #  Many of the standard Docker daemon arguments can be placed under the rancher.docker key. The command needed to start the Docker daemon will be generated based on these arguments. The following arguments are currently supported.\n   Key Value     bridge String   bip String   config_file String   containerd String   debug Boolean   exec_root String   group String   graph String   host List   insecure_registry List   live_restore Boolean   log_driver String   log_opts Map where keys and values are strings   pid_file String   registry_mirror String   restart Boolean   selinux_enabled Boolean   storage_driver String   userland_proxy Boolean    In addition to the standard daemon arguments, there are a few fields specific to BurmillaOS.\n   Key Value Default Description     extra_args List of Strings [] Arbitrary daemon arguments, appended to the generated command   environment List of Strings []    tls Boolean false When setting up TLS, this key needs to be set to true.   tls_args List of Strings (used only if tls: true) []    server_key String (used only if tls: true) \u0026quot;\u0026quot; PEM encoded server TLS key.   server_cert String (used only if tls: true) \u0026quot;\u0026quot; PEM encoded server TLS certificate.   ca_key String (used only if tls: true) \u0026quot;\u0026quot; PEM encoded CA TLS key.   storage_context String console Specifies the name of the system container in whose context to run the Docker daemon process.    Example using extra_args for setting MTU #  The following example can be used to set MTU on the Docker daemon:\n#cloud-config rancher: docker: extra_args: [--mtu, 1460] Example using bip for docker0 bridge #  The docker0 bridge can be configured with docker args, it will take effect after reboot.\n$ ros config set rancher.docker.bip 192.168.0.0/16 Configuring System Docker #  In your cloud-config, System Docker configuration is located under the rancher.system_docker key.\n#cloud-config rancher: system_docker: storage_driver: overlay System Docker settings #  All daemon arguments shown in the first table are also available to System Docker. The following are also supported.\n   Key Value Default Description     extra_args List of Strings [] Arbitrary daemon arguments, appended to the generated command   environment List of Strings (optional) []     The docker-sys bridge can be configured with system-docker args, it will take effect after reboot.\n$ ros config set rancher.system_docker.bip 172.19.0.0/16 The default path of system-docker logs is /var/log/system-docker.log. If you want to write the system-docker logs to a separate partition, e.g. BURMILLA_OEM partition, you can try rancher.defaults.system_docker_logs:\n#cloud-config rancher: defaults: system_docker_logs: /usr/share/ros/oem/system-docker.log Using a pull through registry mirror #  There are 3 Docker engines that can be configured to use the pull-through Docker Hub registry mirror cache:\n#cloud-config rancher: bootstrap_docker: registry_mirror: \u0026quot;http://10.10.10.23:5555\u0026quot; docker: registry_mirror: \u0026quot;http://10.10.10.23:5555\u0026quot; system_docker: registry_mirror: \u0026quot;http://10.10.10.23:5555\u0026quot; bootstrap_docker is used to prepare and initial network and pull any cloud-config options that can be used to configure the final network configuration and System-docker - its very unlikely to pull any images.\nA successful pull through mirror cache request by System-docker looks like:\n[root@burmilla-dev burmilla]# system-docker pull alpine Using default tag: latest DEBU[0201] Calling GET /v1.23/info \u0026gt; WARN[0201] Could not get operating system name: Error opening /usr/lib/os-release: open /usr/lib/os-release: no such file or directory WARN[0201] Could not get operating system name: Error opening /usr/lib/os-release: open /usr/lib/os-release: no such file or directory DEBU[0201] Calling POST /v1.23/images/create?fromImage=alpine%3Alatest DEBU[0201] hostDir: /etc/docker/certs.d/10.10.10.23:5555 DEBU[0201] Trying to pull alpine from http://10.10.10.23:5555/ v2 DEBU[0204] Pulling ref from V2 registry: alpine:latest DEBU[0204] pulling blob \u0026quot;sha256:2aecc7e1714b6fad58d13aedb0639011b37b86f743ba7b6a52d82bd03014b78e\u0026quot; latest: Pulling from library/alpine DEBU[0204] Downloaded 2aecc7e1714b to tempfile /var/lib/system-docker/tmp/GetImageBlob281102233 2aecc7e1714b: Extracting 1.99 MB/1.99 MB DEBU[0204] Untar time: 0.161064213s DEBU[0204] Applied tar sha256:3fb66f713c9fa9debcdaa58bb9858bd04c17350d9614b7a250ec0ee527319e59 to 841c99a5995007d7a66b922be9bafdd38f8090af17295b4a44436ef433a2aecc7e1714b: Pull complete Digest: sha256:0b94d1d1b5eb130dd0253374552445b39470653fb1a1ec2d81490948876e462c Status: Downloaded newer image for alpine:latest Using Multiple User Docker Daemons #  When BurmillaOS is booted, you start with a User Docker service that is running in System Docker. With v1.5.0, BurmillaOS has the ability to create additional User Docker services that can run at the same time.\nTerminology #  Throughout the rest of this documentation, we may simplify to use these terms when describing Docker.\n   Terminology Definition     DinD Docker in docker   User Docker The user-docker on BurmillaOS   Other User Docker The other user-docker daemons you create, these user-docker daemons are automatically assumed to be Docker in Docker.    Pre-Requisites #  User Docker must be set as Docker 17.12.1 or earlier. If it\u0026rsquo;s a later Docker version, it will produce errors when creating a user defined network in System Docker.\n$ ros engine switch docker-17.12.1-ce You will need to create a user-defined network, which will be used when creating the Other User Docker.\n$ system-docker network create --subnet=172.20.0.0/16 dind Create the Other User Docker #  In order to create another User Docker, you will use ros engine create. Currently, BurmillaOS only supports Docker 17.12.1 and 18.03.1 for the Other User Docker image.\n$ ros engine create otheruserdockername --network=dind --fixed-ip=172.20.0.2 After the Other User Docker service is created, users can query this service like other services.\n$ ros service list ... ... disabled volume-efs disabled volume-nfs enabled otheruserdockername You can use ros service up to start the Other User Docker service.\n$ ros service up otheruserdockername After the Other User Docker service is running, you can interact with it just like you can use the built-in User Docker. You would need to append -\u0026lt;SERVICE_NAME\u0026gt; to docker.\n$ docker-otheruserdockername ps -a SSH into the Other User Docker container #  When creating the Other User Docker, you can set an external SSH port so you can SSH into the Other User Docker container in System Docker. By using --ssh-port and adding ssh keys with --authorized-keys, you can set up this optional SSH port.\n$ ros engine create --help ... ... OPTIONS: --ssh-port value --authorized-keys value When using --authorized-keys, you will need to put the key file in one of the following directories:\n/var/lib/rancher/ /opt/ /home/ BurmillaOS will generate a random password for each Other User Docker container, which can be viewed in the container logs. If you do not set any SSH keys, the password can be used.\n$ system-docker logs otheruserdockername ====================================== chpasswd: password for 'root' changed password: xCrw6fEG ====================================== In System Docker, you can SSH into any Other User Docker Container using ssh.\n$ system-docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ca07a25799b burmilla/os-dind:17.12.1 \u0026quot;docker-entrypoint...\u0026quot; 5 seconds ago Up 3 seconds 2375/tcp, 0.0.0.0:34791-\u0026gt;22/tcp otheruserdockername $ ssh -p 34791 root@\u0026lt;HOST_EXTERNAL_IP\u0026gt; $ ssh root@\u0026lt;OTHERUSERDOCKER_CONTAINER_IP\u0026gt; Removing any Other User Docker Service #  We recommend using ros engine rm to remove any Other User Docker service.\n$ ros engine rm otheruserdockername "});index.add({'id':29,'href':'/docs/storage/','title':"Storage",'section':"Docs",'content':"Storage #    Additional Mounts  Custom Partition Layout  Persistent State Partition  Using ZFS  "});index.add({'id':30,'href':'/docs/configuration/kernel/','title':"Kernel",'section':"Configuration",'content':"Kernel #    Kernel boot parameters  Loading Kernel Modules  Installing Kernel Modules that require Kernel Headers  "});index.add({'id':31,'href':'/docs/system-services/','title':"System Services",'section':"Docs",'content':"System Services #  A system service is a container that can be run in either System Docker or Docker. Rancher provides services that are already available in BurmillaOS by adding them to the os-services repo. Anything in the index.yml file from the repository for the tagged release will be an available system service when using the ros service list command.\nEnabling and Starting System Services #  For any services that are listed from the ros service list, they can be enabled by running a single command. After enabling a service, you will need to run start the service.\n# List out available system services $ sudo ros service list disabled amazon-ecs-agent disabled kernel-headers disabled kernel-headers-system-docker disabled open-vm-tools # Enable a system service $ sudo ros service enable kernel-headers # Start a system service $ sudo ros service up kernel-headers Disabling and Removing System Services #  In order to stop a system service from running, you will need to stop and disable the system service.\n# List out available system services $ sudo ros service list disabled amazon-ecs-agent enabled kernel-headers disabled kernel-headers-system-docker disabled open-vm-tools # Disable a system service $ sudo ros service disable kernel-headers # Stop a system service $ sudo ros service stop kernel-headers # Remove the containers associated with the system service $ sudo ros service down kernel-headers If you want to remove a system service from the list of service, just delete the service.\n$ sudo ros service delete \u0026lt;serviceName\u0026gt; "});index.add({'id':32,'href':'/docs/additional-resources/','title':"Aditional Resources",'section':"Docs",'content':"Additional Resources #  Developing #  Development is easiest done with QEMU on Linux. OS X works too, although QEMU doesn\u0026rsquo;t have KVM support. If you are running Linux in a virtual machine, then we recommend you run VMWare Fusion/Workstation and enable VT-x support. Then, QEMU will have KVM support and run sufficiently fast inside your Linux VM.\nBuilding #  Requirements: #   bash make Docker 1.10.3+  $ make The build will run in Docker containers, and when the build is done, the vmlinuz, initrd, and ISO should be in dist/artifacts.\nIf you\u0026rsquo;re building a version of BurmillaOS used for development and not for a release, you can instead run make dev. This will run faster than the standard build by avoiding building the installer.tar and rootfs.tar.gz artifacts which are not needed by QEMU.\nTesting #  Run make integration-tests to run the all integration tests in a container, or ./scripts/integration-tests to run them outside a container (they use QEMU to test the OS.)\nTo run just one integration test, or a group of them (using regex\u0026rsquo;s like .*Console.*, you can set the RUNTEST environment variable:\n$ RUNTEST=TestPreload make integration-test Running #  Prerequisites: QEMU, coreutils, cdrtools/genisoimage/mkisofs. On OS X, brew is recommended to install those. On Linux, use your distro package manager.\nTo launch BurmillaOS in QEMU from your dev version, you can either use make run, or customise the vm using ./scripts/run and its options. You can use --append your.kernel=params here and --cloud-config your-cloud-config.yml to configure the BurmillaOS instance you\u0026rsquo;re launching.\nYou can SSH in using ./scripts/ssh. Your SSH keys should have been populated (if you didn\u0026rsquo;t provide your own cloud-config) so you won\u0026rsquo;t need a password. If you don\u0026rsquo;t have SSH keys, or something is wrong with your cloud-config, then the password is \u0026ldquo;rancher\u0026rdquo;.\nIf you\u0026rsquo;re on OS X, you can run BurmillaOS using xhyve instead of QEMU: just pass --xhyve to ./scripts/run and ./scripts/ssh.\nDebugging and logging. #  You can enable extra log information in the console by setting them using sudo ros config set, or as kernel boot parameters. Enable all logging by setting rancher.debug true or you can set rancher.docker.debug, rancher.system_docker.debug, rancher.bootstrap_docker.debug, or rancher.log individually.\nYou will also be able to view the debug logging information by running dmesg as root.\nRepositories #  All of repositories are located within our main GitHub page.\n BurmillaOS Repo: This repo contains the bulk of the BurmillaOS code.\n BurmillaOS Services Repo: This repo is where any system-services can be contributed.\n BurmillaOS Images Repo: This repo is for the corresponding service images.\nBugs #  If you find any bugs or are having any trouble, please contact us by filing an issue.\nIf you have any updates to our documentation, please make any PRs to our docs repo.\n"});index.add({'id':33,'href':'/docs/faqs/','title':"FAQs",'section':"Docs",'content':"FAQs #  What is required to run BurmillaOS? #  BurmillaOS runs on any laptop, physical, or virtual servers.\nWhat are some commands? #     Command Description     docker Good old Docker, use that to run stuff.   system-docker The Docker instance running the system containers. Must run as root or using sudo   ros Control and configure BurmillaOS    How can I extend my disk size in Amazon? #  Assuming your EC2 instance with BurmillaOS with more disk space than what\u0026rsquo;s being read, run the following command to extend the disk size. This allows BurmillaOS to see the disk size.\n$ docker run --privileged --rm --it debian:jessie resize2fs /dev/xvda1 xvda1 should be the right disk for your own setup. In the future, we will be trying to create a system service that would automatically do this on boot in AWS.\nWhy the name BurmillaOS? #  The \u0026ldquo;Rancher\u0026rdquo; in BurmillaOS\u0026rsquo;s predecessor RancherOS came from the Pets vs Cattle analogy. While RancherOS was founded on the \u0026ldquo;cattle\u0026rdquo; approach, actually, servers often enough end up being pets. Thus, the name Burmilla, a breed of pet cats, was chosen.\n"});index.add({'id':34,'href':'/docs/releases/','title':"Releases",'section':"Docs",'content':"Releases #  The following is a list of the versions of the operating system in their different branches:\nStable #    v2.0.0 | 2024-03-05  Old stable #    v1.9.7-rc1 | 2023-02-01  v1.9.6 | 2023-01-02  Testing #  Archived #    v1.9.5 | 2022-09-15  v1.9.4 | 2022-03-10  v1.9.3 | 2021-10-12  v1.9.2 | 2021-08-04  v1.9.1 | 2021-02-02  v1.9.0 | 2020-12-14  "});index.add({'id':35,'href':'/docs/additional-resources/recovery-console/','title':"Recovery Console",'section':"Aditional Resources",'content':"How to use recovery console #  Test Environment #  In order to demonstrate how to use the recovery console, we choose a scene that the disk space is full and the OS cannot boot.\n   Term Definition     BurmillaOS v2.0.0   Platform Virtualbox   Root Disk 2GB   CPU 1C   MEM 2GB    Fill up the disk #  Start this VM to check disk usage:\n/dev/sda1 ext4 1.8G 567.2M 1.2G 32% /opt /dev/sda1 ext4 1.8G 567.2M 1.2G 32% /mnt ... ... Fill the remaining space with dd:\n$ cd /opt/ $ dd if=/dev/zero of=2GB.img bs=1M count=2000 dd: writing \u0026#39;2GB.img\u0026#39;: No space left on device 1304+0 records in 1302+1 records out $ ls -ahl total 1334036 drwxr-xr-x 2 root root 4.0K Jul 19 07:32 . drwxr-xr-x 1 root root 4.0K Jul 19 06:58 .. -rw-r--r-- 1 root root 1.3G Jul 19 07:32 2GB.img At this point you cannot reboot in the OS, but you can reboot via Virtualbox:\n$ shutdown -h now Failed to write to log, write /var/log/boot/shutdown.log: no space left on device [ ] shutdown:info: Setting shutdown timeout to 60 (rancher.shutdown_timeout set to 60) Failed to write to log, write /var/log/boot/shutdown.log: no space left on device Failed to write to log, write /var/log/boot/shutdown.log: no space left on device .[ ] shutdown:fatal: Error response from daemon: {\u0026#34;message\u0026#34;:\u0026#34;mkdir /var/lib/system-docker/overlay2/7c7dffbed40e7b0ed4c68d5630b17a179751643ca7b7a4ac183e48a767071684-init: no space left on device\u0026#34;} Failed to write to log, write /var/log/boot/shutdown.log: no space left on device After rebooting, you will not be able to enter the OS and there will be a kernel panic.\n Boot with recovery console #  When you can access the bootloader, you should select the Recovery console and press \u0026lt;Tab\u0026gt; to edit.\nYou need add rancher.autologin=tty1 to the end, then press \u0026lt;Enter\u0026gt;. If all goes well, you will automatically login to the recovery console.\nHow to recover #  We need to mount the root disk in the recovery console and delete some data:\n# If you couldn\u0026#39;t see any disk devices created under `/dev/`, please try this command: $ ros udev-settle $ mkdir /mnt/root-disk $ mount /dev/sda1 /mnt/root-disk # delete data previously generated using dd $ ls -ahl /mnt/root-disk/opt -rw-r--r-- 1 root root 1.3G Jul 19 07:32 2GB.img $ rm -f /mnt/root-disk/opt/2GB.img After rebooting, you can enter the OS normally.\n"});index.add({'id':36,'href':'/docs/additional-resources/security/','title':"Security",'section':"Aditional Resources",'content':"BurmillaOS Security #  Security policy #  BurmillaOS is a minimal Linux distribution, built with entirely using open source components.\nReporting process #  Please submit possible security issues by emailing olli.janatuinen@gmail.com\nBurmillaOS Vulnerabilities #     ID Description Date Resolution    "});index.add({'id':37,'href':'/docs/configuration/advanced/airgap-configuration/','title':"Airgap Configuration",'section':"Advanced",'content':"Air Gap Configuration #  In the air gap environment, the Docker registry, BurmillaOS repositories URL, and the BurmillaOS upgrade URL should be configured to ensure the OS can pull images, update OS services, and upgrade the OS.\nConfiguring a Private Docker Registry #  You should use a private Docker registry so that user-docker and system-docker can pull images.\n Add the private Docker registry domain to the images prefix. Set the private registry certificates for user-docker. For details, refer to Certificates for Private Registries Set the private registry certificates for system-docker. There are two ways to set the certificates:   To set the private registry certificates before BurmillaOS starts, you can run a script included with BurmillaOS. For details, refer to Set Custom Certs in ISO. To set the private registry certificates after BurmillaOS starts, append your private registry certs to the /etc/ssl/certs/ca-certificates.crt.rancher file. Then reboot to make the certs fully take effect.  The images used by BurmillaOS should be pushed to your private registry.  Set Custom Certs in ISO #  BurmillaOS provides a script to set your custom certs for an ISO. The following commands show how to use the script:\n$ git clone https://github.com/burmilla/os.git $ cd os $ make shell-bind $ cd scripts/tools/ $ wget http://link/burmillaos-xx.iso $ wget http://link/custom.crt $ ./flush_crt_iso.sh --iso burmillaos-xx.iso --cert custom.crt $ exit $ ls ./build/ Configuring BurmillaOS Repositories and Upgrade URL #  The following steps show how to configure BurmillaOS to update from private repositories.\nBy default, BurmillaOS will update the engine, console, and service list from https://raw.githubusercontent.com/burmilla/os-services and update the os list from https://raw.githubusercontent.com/burmilla/os-services/master/index.yml. So in the air gap environment, you need to change the repository URL and upgrade URL to your own URLs.\n1. Clone os-services files #  Clone github.com/burmilla/os-services to local. The repo has many branches named after the BurmillaOS versions. Please check out the branch that you are using.\n$ git clone https://github.com/burmilla/os-services.git $ cd os-services $ git checkout v1.5.2 2. Download the OS releases yaml #  Download the releases.yml from https://raw.githubusercontent.com/burmilla/os-services/master/index.yml.\n3. Serve these files by HTTP #  Use a HTTP server to serve the cloned os-services directory and download releases.yml. Make sure you can access all the files in os-services and releases.yml by URL.\n4. Set the URLs #  In your cloud-config, set rancher.repositories.core.url and rancher.upgrade.url to your own os-services and releases URLs:\n#cloud-config rancher: repositories: core: url: https://foo.bar.com/os-services upgrade: url: https://foo.bar.com/os/releases.yml You can also customize rancher.repositories.core.url and rancher.upgrade.url after it\u0026rsquo;s been started using ros config.\n$ sudo ros config set rancher.repositories.core.url https://foo.bar.com/os-services $ sudo ros config set rancher.upgrade.url https://foo.bar.com/os/releases.yml Example Cloud-config #  Here is a total cloud-config example for using BurmillaOS in an air gap environment.\nFor system-docker, see Configuring Private Docker Registry.\n#cloud-config write_files: - path: /etc/docker/certs.d/myregistrydomain.com:5000/ca.crt permissions: \u0026#34;0644\u0026#34; owner: root content: |-----BEGIN CERTIFICATE----- MIIDJjCCAg4CCQDLCSjwGXM72TANBgkqhkiG9w0BAQUFADBVMQswCQYDVQQGEwJB VTETMBEGA1UECBMKU29tZS1TdGF0ZTEhMB8GA1UEChMYSW50ZXJuZXQgV2lkZ2l0 cyBQdHkgTHRkMQ4wDAYDVQQDEwVhbGVuYTAeFw0xNTA3MjMwMzUzMDdaFw0xNjA3 MjIwMzUzMDdaMFUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0YXRlMSEw HwYDVQQKExhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQxDjAMBgNVBAMTBWFsZW5h MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxdVIDGlAySQmighbfNqb TtqetENPXjNNq1JasIjGGZdOsmFvNciroNBgCps/HPJphICQwtHpNeKv4+ZuL0Yg 1FECgW7oo6DOET74swUywtq/2IOeik+i+7skmpu1o9uNC+Fo+twpgHnGAaGk8IFm fP5gDgthrWBWlEPTPY1tmPjI2Hepu2hJ28SzdXi1CpjfFYOiWL8cUlvFBdyNqzqT uo6M2QCgSX3E1kXLnipRT6jUh0HokhFK4htAQ3hTBmzcxRkgTVZ/D0hA5lAocMKX EVP1Tlw0y1ext2ppS1NR9Sg46GP4+ATgT1m3ae7rWjQGuBEB6DyDgyxdEAvmAEH4 LQIDAQABMA0GCSqGSIb3DQEBBQUAA4IBAQA45V0bnGPhIIkb54Gzjt9jyPJxPVTW mwTCP+0jtfLxAor5tFuCERVs8+cLw1wASfu4vH/yHJ/N/CW92yYmtqoGLuTsywJt u1+amECJaLyq0pZ5EjHqLjeys9yW728IifDxbQDX0cj7bBjYYzzUXp0DB/dtWb/U KdBmT1zYeKWmSxkXDFFSpL/SGKoqx3YLTdcIbgNHwKNMfTgD+wTZ/fvk0CLxye4P n/1ZWdSeZPAgjkha5MTUw3o1hjo/0H0ekI4erZFrZnG2N3lDaqDPR8djR+x7Gv6E vloANkUoc1pvzvxKoz2HIHUKf+xFT50xppx6wsQZ01pNMSNF0qgc1vvH -----END CERTIFICATE----- rancher: environment: REGISTRY_DOMAIN: xxxx.yyy repositories: core: url: https://foo.bar.com/os-services upgrade: url: https://foo.bar.com/os/releases.yml "});index.add({'id':38,'href':'/docs/configuration/kernel/kernel-modules-kernel-headers/','title':"Kernel Modules Kernel Headers",'section':"Kernel",'content':"Installing Kernel Modules that require Kernel Headers #  To compile any kernel modules, you will need to download the kernel headers. The kernel headers are available in the form of a system service. Since the kernel headers are a system service, they need to be enabled using the ros service command.\nInstalling Kernel Headers #  The following commands can be used to install kernel headers for usage by containers in Docker or System Docker.\nDocker #  $ sudo ros service enable kernel-headers $ sudo ros service up kernel-headers System Docker #  $ sudo ros service enable kernel-headers-system-docker $ sudo ros service up kernel-headers-system-docker The ros service commands will install the kernel headers in /lib/modules/$(uname -r)/build. Based on which service you install, the kernel headers will be available to containers, in Docker or System Docker, by bind mounting specific volumes. For any containers that compile a kernel module, the Docker command will need to bind mount in /usr/src and /lib/modules.\n Note: Since both commands install kernel headers in the same location, the only reason for different services is due to the fact that the storage places for System Docker and Docker are different. Either one or both kernel headers can be installed in the same BurmillaOS services.\n Example of Launching Containers to use Kernel Headers #  # Run a container in Docker and bind mount specific directories $ docker run -it -v /usr/src:/usr/src -v /lib/modules:/lib/modules ubuntu:15.10 # Run a container in System Docker and bind mount specific directories $ sudo system-docker run -it -v /usr/src:/usr/src -v /lib/modules:/lib/modules ubuntu:15.10 "});index.add({'id':39,'href':'/docs/installation/boot-process/','title':"Boot Process",'section':"Installation",'content':"Built-in System Services #  To launch BurmillaOS, we have built-in system services. They are defined in the Docker Compose format, and can be found in the default system config file, /usr/share/ros/os-config.yml. You can add your own system services or override services in the cloud-config.\nPreloading User Images #  Read more about image preloading.\nNetwork #  During this service, networking is set up, e.g. hostname, interfaces, and DNS.\nIt is configured by hostname and rancher.networksettings in cloud-config.\nNTP #  Runs ntpd in a System Docker container.\nConsole #  This service provides the BurmillaOS user interface by running sshd and getty. It completes the BurmillaOS configuration on start up:\n If the rancher.password=\u0026lt;password\u0026gt; kernel parameter exists, it sets \u0026lt;password\u0026gt; as the password for the rancher user. If there are no host SSH keys, it generates host SSH keys and saves them under rancher.ssh.keys in cloud-config. Runs cloud-init -execute, which does the following:  Updates .ssh/authorized_keys in /home/rancher and /home/docker in the cloud-config and metadata. Writes files specified by setting write_files in the cloud-config. Resizes the device specified by setting rancher.resize_device in the cloud-config. Mount devices specified in the mounts in the cloud-config. Set sysctl parameters specified in therancher.sysctl cloud-config.   If user-data contained a file that started with #!, then a file would be saved at /var/lib/rancher/conf/cloud-config-script during cloud-init and then executed. Any errors are ignored. Runs /opt/rancher/bin/start.sh if it exists and is executable. Any errors are ignored. Runs /etc/rc.local if it exists and is executable. Any errors are ignored.  Docker #  This system service runs the user docker daemon. Normally it runs inside the console system container by running docker-init script which, in turn, looks for docker binaries in /opt/bin, /usr/local/bin and /usr/bin, adds the first found directory with docker binaries to PATH and runs dockerlaunch docker daemon appending the passed arguments.\nDocker daemon args are read from rancher.docker.args cloud-config property (followed by rancher.docker.extra_args).\n"});index.add({'id':40,'href':'/docs/installation/boot-process/cloud-init/','title':"Cloud Init",'section':"Boot Process",'content':"Cloud Init #  Userdata and metadata can be fetched from a cloud provider, VM runtime, or management service during the BurmillaOS boot process. Since v0.8.0, this process occurs while BurmillaOS is still running from memory and before System Docker starts. It is configured by the rancher.cloud_init.datasources configuration parameter. For cloud-provider specific images, such as AWS and GCE, the datasource is pre-configured.\nUserdata #  Userdata is a file given by users when launching BurmillaOS hosts. It is stored in different locations depending on its format. If the userdata is a cloud-config file, indicated by beginning with #cloud-config and being in YAML format, it is stored in /var/lib/rancher/conf/cloud-config.d/boot.yml. If the userdata is a script, indicated by beginning with #!, it is stored in /var/lib/rancher/conf/cloud-config-script.\nMetadata #  Although the specifics vary based on provider, a metadata file will typically contain information about the BurmillaOS host and contain additional configuration. Its primary purpose within BurmillaOS is to provide an alternate source for SSH keys and hostname configuration. For example, AWS launches hosts with a set of authorized keys and BurmillaOS obtains these via metadata. Metadata is stored in /var/lib/rancher/conf/metadata.\nConfiguration Load Order #   Cloud-config is read by system services when they need to get configuration. Each additional file overwrites and extends the previous configuration file.\n /usr/share/ros/os-config.yml - This is the system default configuration, which should not be modified by users. /usr/share/ros/oem/oem-config.yml - This will typically exist by OEM, which should not be modified by users. Files in /var/lib/rancher/conf/cloud-config.d/ ordered by filename. If a file is passed in through user-data, it is written by cloud-init and saved as /var/lib/rancher/conf/cloud-config.d/boot.yml. /var/lib/rancher/conf/cloud-config.yml - If you set anything with ros config set, the changes are saved in this file. Kernel parameters with names starting with rancher. /var/lib/rancher/conf/metadata - Metadata added by cloud-init.  "});index.add({'id':41,'href':'/docs/installation/boot-process/image-preloading/','title':"Image Preloading",'section':"Boot Process",'content':"Image Preloading #  On boot, BurmillaOS scans /var/lib/rancher/preload/docker and /var/lib/rancher/preload/system-docker directories and tries to load container image archives it finds there, with docker load and system-docker load.\nThe archives are .tar files, optionally compressed with xz or gzip. These can be produced by docker save command, e.g.:\n$ docker save my-image1 my-image2 some-other/image3 | xz \u0026gt; my-images.tar.xz The resulting files should be placed into /var/lib/rancher/preload/docker or /var/lib/rancher/preload/system-docker (depending on whether you want it preloaded into Docker or System Docker).\nPre-loading process only reads each new archive once, so it won\u0026rsquo;t take time on subsequent boots (\u0026lt;archive\u0026gt;.done files are created to mark the read archives). If you update the archive (place a newer archive with the same name) it\u0026rsquo;ll get read on the next boot as well.\nPre-loading process is asynchronous by default, optionally this can be set to synchronous through the cloud-config file or ros config set command. In the following example, we’ll use the cloud-config file and ros config set command to set BurmillaOS pre-loading process to synchronous.\nAvailable as of RancherOS v1.4\ncloud-config file, e.g.:\n#cloud-config rancher: preload_wait: true ros config set command, e.g.:\n$ ros config set rancher.preload_wait true Pre-packing docker images is handy when you\u0026rsquo;re customizing your BurmillaOS distribution (perhaps, building cloud VM images for your infrastructure).\n"});index.add({'id':42,'href':'/docs/installation/boot-process/logging/','title':"Logging",'section':"Boot Process",'content':"System Logging #  System services #  BurmillaOS uses containers for its system services. This means the logs for syslog, acipd, system-cron, udev, network, ntp, console and the user Docker are available using sudo ros service logs \u0026lt;service-name\u0026gt;.\nBoot logging #  Available as of RancherOS v1.1\nThe init process\u0026rsquo;s logs are copied to /var/log/boot after the user-space filesystem is made available. These can be used to diagnose initialisation, network, and cloud-init issues.\nRemote Syslog logging #  Available as of RancherOS v1.1\nThe Linux kernel has a netconsole logging facility that allows it to send the Kernel level logs to a remote Syslog server.\nTo set up Linux kernel and BurmillaOS remote Syslog logging, you need to set both a local, and remote host IP address - even if this address isn\u0026rsquo;t the final IP address of your system. The kernel setting looks like:\nnetconsole=[+][src-port]@[src-ip]/[\u0026lt;dev\u0026gt;],[tgt-port]@\u0026lt;tgt-ip\u0026gt;/[tgt-macaddr] where + if present, enable extended console support src-port source for UDP packets (defaults to 6665) src-ip source IP to use (interface address) dev network interface (eth0) tgt-port port for logging agent (6666) tgt-ip IP address for logging agent tgt-macaddr ethernet MAC address for logging agent (broadcast) For example, on my current test system, I have set the kernel boot line to:\nprintk.devkmsg=on console=tty1 rancher.autologin=tty1 console=ttyS0 rancher.autologin=ttyS0 rancher.state.dev=LABEL=RANCHER_STATE rancher.state.autoformat=[/dev/sda,/dev/vda] rancher.rm_usr loglevel=8 netconsole=+9999@10.0.2.14/,514@192.168.42.223/ The kernel boot parameters can be set during installation using sudo ros install --append \u0026quot;....\u0026quot;, or on an installed BurmillaOS system, by running sudo ros config syslinux (which will start vi in a container, editing the global.cfg boot config file.\n"});index.add({'id':43,'href':'/docs/installation/cloud/','title':"Cloud",'section':"Installation",'content':"Cloud #    Amazon EC2  Google Compute Engine  DigitalOcean  Azure  OpenStack  VMware ESXi  Aliyun  "});index.add({'id':44,'href':'/docs/installation/cloud/aliyun/','title':"Aliyun",'section':"Cloud",'content':"Aliyun #  Adding the BurmillaOS Image into Aliyun #  BurmillaOS is available as an image in Aliyun, and can be easily run in Elastic Compute Service (ECS). Let’s walk through how to upload the ECS image.\n Download the most recent BurmillaOS image. The image burmillaos-aliyun.vhd can be found in the release artifacts. Follow Aliyun\u0026rsquo;s instructions on how to upload the image. Before the image can be added, it must be uploaded into an OSS bucket. Once the image is added to your ECS, we can start creating new instances!  Example:\n Options #     Option Description     Root disk size The size must be greater than 10GB. Note: When booting the instance, the value must be kept the same.   Platform Select Others Linux   Image Format Select VHD    Launching BurmillaOS using Aliyun Console #  After the image is uploaded, we can use the Aliyun Console to start a new instance. Currently, BurmillaOS on Aliyun only supports SSH key access, so it can only be deployed through the UI.\nSince the image is private, we need to use the Custom Images.\n After the instance is successfully started, we can login with the rancher user via SSH.\n"});index.add({'id':45,'href':'/docs/installation/cloud/azure/','title':"Azure",'section':"Cloud",'content':"Azure #  Because BurmillaOS community is small we do not publish images in Azure. However, you can still use old RancherOS image and simply upgrade to BurmillaOS\nLaunching RancherOS through the Azure Portal #  RancherOS has been published in Azure Marketplace, you can get it from here.\nUsing the new Azure Resource Management portal, click on Marketplace. Search for RancherOS. Click on Create.\nFollow the steps to create a virtual machine.\nIn the Basics step, provide a name for the VM, use rancher as the user name and select the SSH public key option of authenticating. Add your ssh public key into the appropriate field. Select the Resource group that you want to add the VM to or create a new one. Select the location for your VM.\nIn the Size step, select a virtual machine that has at least 1GB of memory.\nIn the Settings step, you can use all the default settings to get RancherOS running.\nReview your VM and buy it so that you can Create your VM.\nAfter the VM has been provisioned, click on the VM to find the public IP address. SSH into your VM using the rancher username.\n$ ssh rancher@\u0026lt;public_ip_of_vm\u0026gt; -p 22 Launching RancherOS with custom data #  Available as of RancherOS v1.5.4\nInstance Metadata Service provides the ability for the VM to have access to its custom data. The binary data must be less than 64 KB and is provided to the VM in base64 encoded form. You can get more details from here\nFor example, you can add custom data through CLI:\n# list images from marketplace az vm image list --location westus --publisher Rancher --offer rancheros --sku os --all --output table Architecture Offer Publisher Sku Urn Version -------------- --------- ----------- ----- ----------------------------- --------- x64 rancheros rancher os rancher:rancheros:os:1.5.1 1.5.1 x64 rancheros rancher os152 rancher:rancheros:os152:1.5.2 1.5.2 x64 rancheros rancher os153 rancher:rancheros:os153:1.5.3 1.5.3 x64 rancheros rancher os154 rancher:rancheros:os154:1.5.4 1.5.4 ... # accept the terms az vm image accept-terms --urn rancher:rancheros:os154:1.5.4 # create the vm AZURE_ROS_SSH_PUBLIC_KEY=\u0026#34;xxxxxx\u0026#34; az vm create --resource-group mygroup \\  --name myvm \\  --image rancher:rancheros:os154:1.5.4 \\  --plan-name os152 \\  --plan-product rancheros \\  --plan-publisher rancher \\  --custom-data ./custom_data.txt \\  --admin-username rancher \\  --size Standard_A1 \\  --ssh-key-value \u0026#34;$AZURE_ROS_SSH_PUBLIC_KEY\u0026#34; The custom_data.txt can be the cloud-config format or a shell script, such as:\n#cloud-config runcmd: - [ touch, /home/rancher/test1 ] - echo \u0026#34;test\u0026#34; \u0026gt; /home/rancher/test2 #!/bin/sh echo \u0026quot;aaa\u0026quot; \u0026gt; /home/rancher/aaa.txt "});index.add({'id':46,'href':'/docs/installation/cloud/digital-ocean/','title':"Digital Ocean",'section':"Cloud",'content':"Digital Ocean #  BurmillaOS is available in the Digital Ocean portal. BurmillaOS is a member of container distributions and you can find it easily.\n Note Deploying to Digital Ocean will incur charges.\n To start a BurmillaOS Droplet on Digital Ocean:\n In the Digital Ocean portal, go to the project view. Click New Droplet. Click Create Droplet. Click the Container distributions tab. Click BurmillaOS. Choose a plan. Make sure your Droplet has the minimum hardware requirements for BurmillaOS. Choose any options for backups, block storage, and datacenter region. Optional: In the Select additional options section, you can check the User data box and enter a cloud-config file in the text box that appears. The cloud-config file is used to provide a script to be run on the first boot. An example is below. Choose an SSH key that you have access to, or generate a new SSH key. Choose your project. Click Create.  You can access the host via SSH after the Droplet is booted. The default user is rancher.\nBelow is an example cloud-config file that you can use to initialize the Droplet with user data, such as deploying Rancher:\n#cloud-config write_files: - path: /etc/rc.local permissions: \u0026#34;0755\u0026#34; owner: root content: |#!/bin/bash wait-for-docker export curlimage=appropriate/curl export jqimage=stedolan/jq export burmilla_version=v2.2.2 for image in $curlimage $jqimage \u0026#34;burmilla/burmilla:${burmilla_version}\u0026#34;; do until docker inspect $image \u0026gt; /dev/null 2\u0026gt;\u0026amp;1; do docker pull $image sleep 2 done done docker run -d --restart=unless-stopped -p 80:80 -p 443:443 -v /opt/rancher:/var/lib/rancher burmilla/burmilla:${burmilla_version} "});index.add({'id':47,'href':'/docs/installation/custom-builds/','title':"Custom Builds",'section':"Installation",'content':"Custom Builds #    Custom BurmillaOS ISO  Custom Console  Custom Kernels  "});index.add({'id':48,'href':'/docs/installation/custom-builds/custom-console/','title':"Custom Console",'section':"Custom Builds",'content':"Custom Console #  When booting from the ISO, BurmillaOS starts with the default console, which is based on debian:buster-slim.\nNo other console is officially supported.\nUsing the unofficial custom console images #  If you want to use other console, the most easy way is use the unofficial custom console images.\nTo use this, you need to add the following settings to your cloud-init.yml.\nrancher: repositories: console: url: https://raw.githubusercontent.com/benok/burmilla-os-console/master With the settings above, you can select which console you want BurmillaOS to start with using the cloud-config.\nIf you want to create your own custom console, please check this page.\nEnabling Consoles using Cloud-Config #  When launching BurmillaOS with a cloud-config file, you can select which console you want to use.\nCurrently, the list of available consoles (with above setting using the unofficial console images) are:\n official image  default (debian:buster-slim)   unofficial images  debian (debian:buster) debian_testing (debian:testing) ubuntu (ubuntu:latest) alpine (alpine:latest) fedora (fedora:latest)    Here is an example cloud-config file that can be used to enable the debian console.\n#cloud-config rancher: console: debian Listing Available Consoles #  You can easily list the available consoles in BurmillaOS and what their status is with sudo ros console list.\n$ sudo ros console list disabled alpine disabled debian disabled debian_testing enabled default disabled fedora disabled ubuntu Changing Consoles after BurmillaOS has started (not recomemded) #  ros console switch has several bugs since RancherOS era, please use \u0026ldquo; Enabling consoles\u0026rdquo; below.\nYou can view which console is being used by BurmillaOS by checking which console container is running in System Docker. If you wanted to switch consoles, you just need to run a simple command and select your new console.\nFor our example, we\u0026rsquo;ll switch to the Ubuntu console.\n$ sudo ros console switch ubuntu Switching consoles will 1. destroy the current console container 2. log you out 3. restart Docker Continue [y/N]:y Pulling console (burmilla/os-ubuntuconsole:v0.5.0-3)... v0.5.0-3: Pulling from burmilla/os-ubuntuconsole 6d3a6d998241: Pull complete 606b08bdd0f3: Pull complete 1d99b95ffc1c: Pull complete a3ed95caeb02: Pull complete 3fc2f42db623: Pull complete 2fb84911e8d2: Pull complete fff5d987b31c: Pull complete e7849ae8f782: Pull complete de375d40ae05: Pull complete 8939c16614d1: Pull complete Digest: sha256:37224c3964801d633ea8b9629137bc9d4a8db9d37f47901111b119d3e597d15b Status: Downloaded newer image for burmilla/os-ubuntuconsole:v0.5.0-3 switch-console_1 | time=\u0026quot;2016-07-02T01:47:14Z\u0026quot; level=info msg=\u0026quot;Project [os]: Starting project \u0026quot; switch-console_1 | time=\u0026quot;2016-07-02T01:47:14Z\u0026quot; level=info msg=\u0026quot;[0/18] [console]: Starting \u0026quot; switch-console_1 | time=\u0026quot;2016-07-02T01:47:14Z\u0026quot; level=info msg=\u0026quot;Recreating console\u0026quot; Connection to 127.0.0.1 closed by remote host. After logging back, you\u0026rsquo;ll be in the Ubuntu console.\n$ sudo system-docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6bf33541b2dc burmilla/os-ubuntuconsole:v0.5.0-rc3 \u0026quot;/usr/sbin/entry.sh /\u0026quot; About a minute ago Up About a minute  Note: When switching between consoles, the currently running console container is destroyed, Docker is restarted and you will be logged out.\n Console persistence #  All consoles except the default (busybox) console are persistent. Persistent console means that the console container will remain the same and preserves changes made to its filesystem across reboots. If a container is deleted/rebuilt, state in the console will be lost except what is in the persisted directories.\n/home /opt /var/lib/docker /var/lib/rancher  Note: When using a persistent console and in the current version\u0026rsquo;s console, rolling back is not supported. For example, rolling back to v0.4.5 when using a v0.5.0 persistent console is not supported.\n Enabling Consoles #  You can also enable a console that will be changed at the next reboot.\nFor our example, we\u0026rsquo;ll switch to the Debian console.\n# Check the console running in System Docker $ sudo system-docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 95d548689e82 burmilla/os-docker:v0.5.0 \u0026quot;/usr/sbin/entry.sh /\u0026quot; About an hour ago Up About an hour docker # Enable the Debian console $ sudo ros console enable debian Pulling console (burmilla/os-debianconsole:v0.5.0-3)... v0.5.0-3: Pulling from burmilla/os-debianconsole 7268d8f794c4: Pull complete a3ed95caeb02: Pull complete 21cb8a645d75: Pull complete 5ee1d288a088: Pull complete c09f41c2bd29: Pull complete 02b48ce40553: Pull complete 38a4150e7e9c: Pull complete Digest: sha256:5dbca5ba6c3b7ba6cd6ac75a1d054145db4b4ea140db732bfcbd06f17059c5d0 Status: Downloaded newer image for burmilla/os-debianconsole:v0.5.0-3 At the next reboot, BurmillaOS will be using the Debian console.\n"});index.add({'id':49,'href':'/docs/installation/custom-builds/custom-kernels/','title':"Custom Kernels",'section':"Custom Builds",'content':"Custom Kernels #  Kernel version in BurmillaOS #  BurmillaOS basically uses the standard Linux kernel, but we maintain a kernel config ourselves. Due to various feature support and security fixes, we are constantly updating the kernel version.\n   BurmillaOS Kernel     =v2.0.x 5.9.x   =v1.9.x 4.14.x    Building and Packaging a Kernel to be used in BurmillaOS #  We build the kernel for BurmillaOS at the os-kernel repository. You can use this repository to help package your own custom kernel to be used in BurmillaOS.\nCreate a clone of the os-kernel repository to your local machine using git clone.\n$ git clone https://github.com/burmilla/os-kernel.git If you want to build kernel v4.14.53, you can refer to the following command. After the build is completed, a ./dist/kernel directory will be created with the freshly built kernel tarball and headers.\n$ git tag v4.14.53-burmilla $ KERNEL_TAG=4.14.53 make release ...snip... ./dist/kernel/extra-linux-4.14.53-burmilla-x86.tar.gz ./dist/kernel/build-linux-4.14.53-burmilla-x86.tar.gz ./dist/kernel/linux-4.14.53-burmilla-x86.tar.gz ./dist/kernel/config ...snip... Images ready to push: burmilla/os-extras:4.14.53-burmilla burmilla/os-headers:4.14.53-burmilla For some users who need a custom kernel, the following information is very useful to you:\n The modules defined in modules.list will be packaged into the built-in modules. The modules defined in modules-extra.list will be packaged into the extra modules. You can modify config/kernel-config to build the kernel modules you need. You can add your patches in the patches directory, and os-kernel will update these patches after downloading the kernel source.  Now you need to either upload the ./dist/kernel/linux-4.14.53-burmilla-x86.tar.gz file to somewhere, or copy that file into your clone of the burmilla/os repo, as assets/kernel.tar.gz.\nThe build-\u0026lt;name\u0026gt;.tar.gz and extra-\u0026lt;name\u0026gt;.tar.gz files are used to build the burmilla/os-extras and burmilla/os-headers images for your BurmillaOS release - which you will need to tag them with a different organisation name, push them to a registry, and create custom service.yml files.\nYour kernel should be packaged and published as a set of files of the following format:\n  \u0026lt;kernel-name-and-version\u0026gt;.tar.gz is the one KERNEL_URL in burmilla/os should point to. It contains the kernel binary, core modules and firmware.\n  build-\u0026lt;kernel-name-and-version\u0026gt;.tar.gz contains build headers to build additional modules: it is a subset of the kernel sources tarball. These files will be installed into /usr/src/\u0026lt;os-kernel-tag\u0026gt; using the kernel-headers-system-docker and kernel-headers services.\n  extra-\u0026lt;kernel-name-and-version\u0026gt;.tar.gz contains extra modules and firmware for your kernel and should be built into a kernel-extras service.\n  Building a BurmillaOS release using the Packaged kernel files. #  By default, BurmillaOS ships with the kernel provided by the os-kernel repository. Swapping out the default kernel can by done by building your own custom BurmillaOS ISO.\nCreate a clone of the main BurmillaOS repository to your local machine with a git clone.\n$ git clone https://github.com/burmilla/os.git In the root of the repository, the \u0026ldquo;General Configuration\u0026rdquo; section of Dockerfile.dapper will need to be updated. Using your favorite editor, replace the appropriate KERNEL_URL value with a URL of your compiled custom kernel tarball. Ideally, the URL will use HTTPS.\n# Update the URL to your own custom kernel tarball ARG KERNEL_VERSION_amd64=4.14.63-burmilla ARG KERNEL_URL_amd64=https://link/xxxx After you\u0026rsquo;ve replaced the URL with your custom kernel, you can follow the steps in building your own custom BurmillaOS ISO.\n Note: KERNEL_URL settings should point to a Linux kernel, compiled and packaged in a specific way. You can fork os-kernel repository to package your own kernel.\n "});index.add({'id':50,'href':'/docs/installation/server/','title':"Server",'section':"Installation",'content':"Bare Metal \u0026amp; Virtual Servers #    PXE  Install to Hard Disk  Raspberry Pi  "});index.add({'id':51,'href':'/docs/installation/server/install-to-disk/','title':"Install to Disk",'section':"Server",'content':"Installing to Disk #  BurmillaOS comes with a simple installer that will install BurmillaOS on a given target disk. To install BurmillaOS on a new disk, you can use the ros install command. Before installing, you\u0026rsquo;ll need to have already booted BurmillaOS from ISO. Please be sure to pick the burmillaos.iso from our release page.\nInstalling BurmillaOS #  The ros install command orchestrates the installation from the burmilla/os container. You will need to have already created a cloud-config file and found the target disk.\nCloud-Config #  The easiest way to log in is to pass a cloud-config.yml file containing your public SSH keys. To learn more about what\u0026rsquo;s supported in our cloud-config, please read our documentation.\nThe ros install command will process your cloud-config.yml file specified with the -c flag. This file will also be placed onto the disk and installed to /var/lib/rancher/conf/. It will be evaluated on every boot.\nCreate a cloud-config file with a SSH key, this allows you to SSH into the box as the rancher user. The yml file would look like this:\n#cloud-config ssh_authorized_keys: - ssh-rsa AAA... You can generate a new SSH key for cloud-config.yml file by following this article.\nCopy the public SSH key into BurmillaOS before installing to disk.\nNow that our cloud-config.yml contains our public SSH key, we can move on to installing BurmillaOS to disk!\n$ sudo ros install -c cloud-config.yml -d /dev/sda INFO[0000] No install type specified...defaulting to generic Installing from burmilla/os:v2.0.0 Continue [y/N]: For the cloud-config.yml file, you can also specify a remote URL, but you need to make sure you can get it:\n$ sudo ros install -c https://link/to/cloud-config.yml You will be prompted to see if you want to continue. Type y.\nUnable to find image \u0026#39;burmilla/os:v2.0.0\u0026#39; locally v2.0.0: Pulling from burmilla/os ... ... ... Status: Downloaded newer image for burmilla/os:v2.0.0 + DEVICE=/dev/sda ... ... ... + umount /mnt/new_img Continue with reboot [y/N]: After installing BurmillaOS to disk, you will no longer be automatically logged in as the rancher user. You\u0026rsquo;ll need to have added in SSH keys within your cloud-config file.\nInstalling a Different Version #  By default, ros install uses the same installer image version as the ISO it is run from. The -i option specifies the particular image to install from. To keep the ISO as small as possible, the installer image is downloaded from DockerHub and used in System Docker. For example for BurmillaOS v2.0.0 the default installer image would be burmilla/os:v2.0.0.\nYou can use ros os list command to find the list of available BurmillaOS images/versions.\n$ sudo ros os list burmilla/os:v2.0.0 local Alternatively, you can set the installer image to any image in System Docker to install BurmillaOS. This is particularly useful for machines that will not have direct access to the internet.\nCaching Images #  Available as of RancherOS v1.5.3\nSome configurations included in cloud-config require images to be downloaded from Docker to start. After installation, these images are downloaded automatically by BurmillaOS when booting. An example of these configurations are:\n rancher.services_include rancher.console rancher.docker  If you want to download and save these images to disk during installation, they will be cached and not need to be downloaded again upon each boot. You can cache these images by adding -s when using ros install:\n$ ros install -d \u0026lt;disk\u0026gt; -c \u0026lt;cloud-config.yaml\u0026gt; -s SSH into BurmillaOS #  After installing BurmillaOS, you can ssh into BurmillaOS using your private key and the rancher user.\n$ ssh -i /path/to/private/key rancher@\u0026lt;ip-address\u0026gt; Installing with no Internet Access #  If you\u0026rsquo;d like to install BurmillaOS onto a machine that has no internet access, it is assumed you either have your own private registry or other means of distributing docker images to System Docker of the machine. If you need help with creating a private registry, please refer to the Docker documentation for private registries.\nIn the installation command (i.e. sudo ros install), there is an option to pass in a specific image to install. As long as this image is available in System Docker, then BurmillaOS will use that image to install BurmillaOS.\n$ sudo ros install -c cloud-config.yml -d /dev/sda -i \u0026lt;Image_Name_in_System_Docker\u0026gt; INFO[0000] No install type specified...defaulting to generic Installing from \u0026lt;Image_Name_in_System_Docker\u0026gt; Continue [y/N]: "});index.add({'id':52,'href':'/docs/installation/server/raspberry-pi/','title':"Raspberry Pi",'section':"Server",'content':"Raspberry Pi #  BurmillaOS releases include a Raspberry Pi image that can be found on our releases page. The official Raspberry Pi documentation contains instructions on how to install operating system images.\nWhen installing, there is no ability to pass in a cloud-config. You will need to boot up, change the configuration and then reboot to apply those changes.\nCurrently, only Raspberry Pi 3 is tested and known to work.\n Note: It is not necessary to run ros install after installing BurmillaOS to an SD card.\n Using the entire SD Card #  BurmillaOS does not currently expand the root partition to fill the remainder of the SD card automatically. Instead, the following workaround can be used to store Docker containers on a larger partition that fills the remainder.\n sudo fdisk /dev/mmcblk0 Create a new partition Press [Enter] four (4x) times to accept the defaults Then write the table and exit sudo reboot to reboot and reload the new partition table sudo mkdir /mnt/docker to create the directory to be used as the new Docker root sudo ros config set rancher.docker.extra_args [-g,/mnt/docker] to configure Docker to use the new root sudo mkfs.ext4 /dev/mmcblk0p3 to format the disk sudo ros config set mounts \u0026quot;[['/dev/mmcblk0p3','/mnt/docker','ext4','']]\u0026quot; to preserve this mount after reboots sudo mount /dev/mmcblk0p3 /mnt/docker to mount the Docker root sudo system-docker restart docker to restart Docker using the new root If this is not a new installation, you\u0026rsquo;ll have to copy over your existing Docker root (/var/lib/docker) to the new root (/mnt/docker). sudo cp -R /var/lib/docker/* /mnt/docker to recursively copy all files sudo system-docker restart docker to restart Docker using the new root  Using Wi-Fi #  Available as of RancherOS v1.5.2\nHere are steps about how to enable Wi-Fi on a Raspberry Pi:\nmodprobe brcmfmac wpa_passphrase \u0026lt;ssid\u0026gt; \u0026lt;psk\u0026gt; \u0026gt; /etc/wpa_supplicant.conf wpa_supplicant -iwlan0 -B -c /etc/wpa_supplicant.conf # wait a few seconds, then dhcpcd -MA4 wlan0 You can also use cloud-config to enable Wi-Fi:\n#cloud-config rancher: network: interfaces: wlan0: wifi_network: network1 wifi_networks: network1: ssid: \u0026quot;Your wifi ssid\u0026quot; psk: \u0026quot;Your wifi password\u0026quot; scan_ssid: 1 Raspberry Pi will automatically drop Wi-Fi connection after a while, this is due to power management. To fix this problem, you can try this:\niwconfig wlan0 power off "});index.add({'id':53,'href':'/docs/installation/workstation/','title':"Workstation",'section':"Installation",'content':"Workstation #    Docker Machine  Boot from ISO  Apple Silicon  "});index.add({'id':54,'href':'/docs/installation/workstation/apple-silicon/','title':"Apple Silicon",'section':"Workstation",'content':"Apple Silicon #  On MacOS, particularly on Apple Silicon (M1, M2, M3, \u0026hellip;), BurmillaOS can be tested using UTM. To run BurmillaOS on UTM, create a new virtual machine using \u0026ldquo;Emulate\u0026rdquo; CPU, for a \u0026ldquo;Linux\u0026rdquo; operating system, and set the CPU architecture to \u0026ldquo;x86_64\u0026rdquo;. Make sure to disable UEFI Boot and use the virtio-vga display card emulation in the settings.\n"});index.add({'id':55,'href':'/docs/installation/workstation/boot-from-iso/','title':"Boot From Iso",'section':"Workstation",'content':"Booting from ISO #  The BurmillaOS ISO file can be used to create a fresh BurmillaOS install on KVM, VMware, VirtualBox, Hyper-V, Proxmox VE, or bare metal servers. You can download the burmillaos.iso file from our releases page.\nSome hypervisors may require a built-in agent to communicate with the guest, for this, BurmillaOS precompiles some ISO files.\n   Hypervisor ISO     VMware burmillaos-vmware.iso   Hyper-V burmillaos-hyperv.iso   Proxmox VE burmillaos-proxmoxve.iso    You must boot with enough memory which you can refer to here. If you boot with the ISO, you will automatically be logged in as the rancher user. Only the ISO is set to use autologin by default. If you run from a cloud or install to disk, SSH keys or a password of your choice is expected to be used.\nInstall to Disk #  After you boot BurmillaOS from ISO, you can follow the instructions here to install BurmillaOS to a hard disk.\n"});index.add({'id':56,'href':'/docs/installation/workstation/docker-machine/','title':"Docker Machine",'section':"Workstation",'content':"Using Docker Machine #  Before we get started, you\u0026rsquo;ll need to make sure that you have docker machine installed. Download it directly from the docker machine releases. You also need to know the memory requirements.\n Note: If you create a BurmillaOS instance using Docker Machine, you will not be able to upgrade your version of BurmillaOS.\n Downloading BurmillaOS #  Get the latest ISO artifact from the BurmillaOS releases.\n   Machine Driver Recommended BurmillaOS version ISO File     VirtualBox \u0026gt;=v2.0.0 burmillaos.iso   VMWare VSphere \u0026gt;=v2.0.0 burmillaos-autoformat.iso   VMWare Fusion \u0026gt;=v2.0.0 burmillaos-autoformat.iso   Hyper-V \u0026gt;=v2.0.0 burmillaos.iso   Proxmox VE \u0026gt;=v2.0.0 burmillaos-autoformat.iso    Docker Machine #  You can use Docker Machine to launch VMs for various providers. Currently VirtualBox and VMWare(VMWare VSphere, VMWare Fusion) and AWS are supported.\nUsing VirtualBox #  Before moving forward, you\u0026rsquo;ll need to have VirtualBox installed. Download it directly from VirtualBox. Once you have VirtualBox and Docker Machine installed, it\u0026rsquo;s just one command to get BurmillaOS running.\nHere is an example about using the BurmillaOS latest link:\n$ docker-machine create -d virtualbox \\  --virtualbox-boot2docker-url https://github.com/burmilla/os/releases/download/\u0026lt;version\u0026gt;/burmillaos.iso \\  --virtualbox-memory \u0026lt;MEMORY-SIZE\u0026gt; \\  \u0026lt;MACHINE-NAME\u0026gt;  Note: Instead of downloading the ISO, you can directly use the URL for the burmillaos.iso.\n That\u0026rsquo;s it! You should now have a BurmillaOS host running on VirtualBox. You can verify that you have a VirtualBox VM running on your host.\n Note: After the machine is created, Docker Machine may display some errors regarding creation, but if the VirtualBox VM is running, you should be able to log in.\n $ VBoxManage list runningvms | grep \u0026lt;MACHINE-NAME\u0026gt; This command will print out the newly created machine. If not, something went wrong with the provisioning step.\nUsing VMWare VSphere #  Before moving forward, you’ll need to have VMWare VSphere installed. Once you have VMWare VSphere and Docker Machine installed, it’s just one command to get BurmillaOS running.\nHere is an example about using the BurmillaOS latest link:\n$ docker-machine create -d vmwarevsphere \\  --vmwarevsphere-username \u0026lt;USERNAME\u0026gt; \\  --vmwarevsphere-password \u0026lt;PASSWORD\u0026gt; \\  --vmwarevsphere-memory-size \u0026lt;MEMORY-SIZE\u0026gt; \\  --vmwarevsphere-boot2docker-url https://github.com/burmilla/os/releases/download/\u0026lt;version\u0026gt;/burmillaos-autoformat.iso \\  --vmwarevsphere-vcenter \u0026lt;IP-ADDRESS\u0026gt; \\  --vmwarevsphere-vcenter-port \u0026lt;PORT\u0026gt; \\  --vmwarevsphere-disk-size \u0026lt;DISK-SIZE\u0026gt; \\  \u0026lt;MACHINE-NAME\u0026gt; That’s it! You should now have a BurmillaOS host running on VMWare VSphere. You can verify that you have a VMWare(ESXi) VM running on your host.\nUsing VMWare Fusion #  Before moving forward, you’ll need to have VMWare Fusion installed. Once you have VMWare Fusion and Docker Machine installed, it’s just one command to get BurmillaOS running.\nHere is an example about using the BurmillaOS latest link:\n$ docker-machine create -d vmwarefusion \\  --vmwarefusion-no-share \\  --vmwarefusion-memory-size \u0026lt;MEMORY\u0026gt; \\  --vmwarefusion-boot2docker-url https://github.com/burmilla/os/releases/download/\u0026lt;version\u0026gt;/burmillaos-autoformat.iso \\  \u0026lt;MACHINE_NAME\u0026gt; That’s it! You should now have a BurmillaOS host running on VMWare Fusion. You can verify that you have a VMWare Fusion VM running on your host.\nUsing Hyper-V #  You should refer to the documentation of Hyper-V driver, here is an example of using the latest BurmillaOS URL. We recommend using a specific version so you know which version of BurmillaOS that you are installing.\n$ docker-machine.exe create -d hyperv \\  --hyperv-memory 2048 \\  --hyperv-boot2docker-url https://github.com/burmilla/os/releases/download/\u0026lt;version\u0026gt;/burmillaos.iso --hyperv-virtual-switch \u0026lt;SWITCH_NAME\u0026gt; \\  \u0026lt;MACHINE_NAME\u0026gt; Using Proxmox VE #  There is currently no official Proxmox VE driver, but there is a choice that you can refer to.\nLogging into BurmillaOS #  Logging into BurmillaOS follows the standard Docker Machine commands. To login into your newly provisioned BurmillaOS VM.\n$ docker-machine ssh \u0026lt;MACHINE-NAME\u0026gt; You\u0026rsquo;ll be logged into BurmillaOS and can start exploring the OS, This will log you into the BurmillaOS VM. You\u0026rsquo;ll then be able to explore the OS by adding system services, customizing the configuration, and launching containers.\nIf you want to exit out of BurmillaOS, you can exit by pressing Ctrl+D.\nDocker Machine Benefits #  With Docker Machine, you can point the docker client on your host to the docker daemon running inside of the VM. This allows you to run your docker commands as if you had installed docker on your host.\nTo point your docker client to the docker daemon inside the VM, use the following command:\n$ eval $(docker-machine env \u0026lt;MACHINE-NAME\u0026gt;) After setting this up, you can run any docker command in your host, and it will execute the command in your BurmillaOS VM.\n$ docker run -p 80:80 -p 443:443 -d nginx In your VM, a nginx container will start on your VM. To access the container, you will need the IP address of the VM.\n$ docker-machine ip \u0026lt;MACHINE-NAME\u0026gt; Once you obtain the IP address, paste it in a browser and a Welcome Page for nginx will be displayed.\n"});index.add({'id':57,'href':'/docs/reference/','title':"Reference",'section':"Docs",'content':"Reference #  ros Commands #     Commands / Options Description Example      --version / -v print the version ros -v    --help / -h show help ros -h        os operating system upgrade/downgrade  see below   config configure settings  see below   console manage which console container is used  see below   install install BurmillaOS to disk  see below   engine manage which Docker engine is used  see below   service   see below   tls setup tls configuration  see below    ros os Sub-Commands #     Commands / Options Description Example     version show the currently installed version ros os version        list list the current available versions ros os list    --update / -u update engine cache ros os list --update        upgrade upgrade to the latest version ros os upgrade    --image / -i upgrade to a certain image ros os upgrade --image value    --stage / -s only stage the new upgrade, don\u0026rsquo;t apply it ros os upgrade --stage    --force / -f do not prompt for input ros os upgrade --force    --kexec / -k reboot using kexec ros os upgrade --kexec    --no-reboot do not reboot after update ros os upgrade --no-reboot    --append append additional kernel parameters ros os upgrade --append value    --upgrade-console upgrade console even if persistent ros os upgrade --upgrade-console    --debug run installer with debug output ros os upgrade --debug    ros config Sub-Commands #     Commands / Options Description Example     get get value ros config get value        set set a value ros config set value        images list Docker images for a configuration from a file ros config images    --input / -i file from which to read config ros config images --input value        generate generate a configuration file from a template ros config generate        merge merge configuration from stdin ros config merge    --input / -i file from which to read ros config merge --input value        export export configuration ros config export    --output / -o file to which to save ros config export --output value    --private / -p include the generated private keys ros config export --private    --full / -f export full configuration, including internal and default settings ros config export --full        validate validate configuration form stdin ros config validate    --input / -i file from which to read ros config validate --input value        syslinux edit Syslinux boot global.cfg ros config syslinux    ros console Sub-Commands #     Commands / Options Description Example     list list available consoles ros console list    --update / -u update engine cache ros console list --update        enable set console to be switched on next boot ros console enable default        switch switch console without a reboot ros console switch default    --force / -f do not prompt for input ros console switch --force default    --no-pull don\u0026rsquo;t pull console image ros console switch --no-pull default    Available Consoles #     Console Image Description     default debian:buster-slim Debian 10 \u0026ldquo;Buster\u0026rdquo; (slimmer base)   debian debian:buster Debian 10 \u0026ldquo;Buster\u0026rdquo;   debian_testing debian:testing Debian 10 \u0026ldquo;Buster\u0026rdquo; (more recent software)   ubuntu ubuntu:latest  Ubuntu \u0026ldquo;latest\u0026rdquo;   alpine alpine:latest  Alpine \u0026ldquo;latest\u0026rdquo;   fedora fedora:latest  Fedora \u0026ldquo;latest\u0026rdquo;    ros install Sub-Command #     Command / Options Description Example     install Install BurmillaOS to disk ros install    --cloud-config / -c cloud-config yml file - needed for SSH authorized keys ros install --cloud-config value    --device / -d storage device ros install --device value    --image / -i install from a certain image ros install --image value    --save / -s save services and images for next booting ros install --save    --install-type / -t generic (default), amazon-ebs, gptsyslinux ros install --install-type value    --force / -f [DANGEROUS! Data loss can happen] partition/format without prompting ros install --force    --partition / -p partition to install to ros install --partition value    --append / -a append additional kernel parameters ros install --append value    --debug run installer with debug output ros install --debug    --statedir value install to rancher.state.directory ros install --statedir value    --kexec / -k reboot using kexec ros install --kexec    --no-reboot do not reboot after install ros install --no-reboot    --rollback / -r rollback version ros install --rollback    ros engine Sub-Commands #     Commands / Options Description Example     list list available Docker engines (include the DinD engines) ros engine list    --update / -u update engine cache ros engine list --update        switch switch user Docker engine without reboot ros engine switch current    --force / -f do not prompt for input ros engine switch --force current    --no-pull don\u0026rsquo;t pull engine image ros engine switch --no-pull current        enable set user Docker engine to be switched on next boot ros engine enable current        create create DinD engine without a reboot ros engine create my_name    --version / -v set version for the engine ros engine create --version value my_name    --network set the network for the engine ros engine create --network value my_name    --fixed-ip set the fixed ip for the engine ros engine create --fixed-ip value my_name    --ssh-port set the ssh port for the engine ros engine create --ssh-port value my_name    --authorized-keys set the authorized_keys absolute path for the engine ros engine create --authorized-keys my_name        rm remove DinD engine without a reboot ros engine rm my_name    --force / -f do not prompt for input ros engine rm --force my_name    --timeout / -t specify a shutdown timeout in seconds (default: 10) ros engine rm --timout 10 my_name    ros service Sub-Commands #     Commands / Options Description Example     list list services and state ros service list    --update / -u update service cache ros service list --update    --all / -a list all services and state ros service list --all        up create and start containers ros service up    --foreground run in foreground and log ros service up --foreground    --no-build don\u0026rsquo;t build an image, even if it\u0026rsquo;s missing ros service up --no-build    --no-recreate if containers already exists, don\u0026rsquo;t recreate them. Incompatible with --force-recreate ros service up --no-recreate    --force-recreate recreate containers even if their configuration and imager haven\u0026rsquo;t changed. Incompatible with no-recreate ros service up --force-recreate        down stop and remove containers, networks, images, and volumes ros service down    --volumes / -v remove data volume ros service down --volumes    --rmi remove images, type may be one of: \u0026lsquo;all\u0026rsquo; to remove all images, or \u0026lsquo;local\u0026rsquo; to remove only images that don\u0026rsquo;t have an custom name set by the image field ros service down --rmi value    --remove-orphans remove containers for services not defined in the Compose file ros service down --remove orphans        enable turn on a service ros service enable        disable turn off a service ros service disable        rm delete services ros service rm    --force / -f allow deletion of all services ros service rm --force    -v remove volumes associated with containers ros service rm -v        delete delete a service ros service delete        logs view output from containers ros service logs    --lines number of lines to tail (default: 100) ros service logs --lines    --follow / -f follow log output ros service logs --follow        build build or rebuild services ros service build    --no-cache do not use cache when building the image ros service build --no cache    --force-rm always remove intermediate containers ros service build --force-rm    --pull alwys attempt to pull a newer version of the image ros service build --pull        create create services ros service create    --no-recreate if containers already exist, don\u0026rsquo;t recreate them. Incompatible with --force-recreate ros service create --no-recreate    --force-recreate recreate containers even if their configuration and images haven\u0026rsquo;t changed. Incompatible with --no-recreate ros service create --force-recreate    --no-build don\u0026rsquo;t build an image, even if it is missing ros service create --no-build        start start services ros service start    --foreground run in foreground and log ros service start --foreground        restart restart services ros service restart    --timeout / -t specify a shutdown timeout in seconds (default: 10) ros service restart --timeout 10        stop stop services ros service stop    --timeout / -t specify a shutdown timeout in seconds (default: 10) ros service stop --timeout 10        kill kill containers ros service kill    --signal / -s SIGAL to send to the container (default: \u0026ldquo;SiGKILL\u0026rdquo;) ros service kill --signal value        pull pulls service images ros service pull    --ignore-pull-failures pull what it can and ignores images with pull failures ros service pull ----ignore-pull-failures        ps list containers ros service ps    -q only display IDs ros service ps -q    ros tls Sub-Command #     Command / Options Description Example     generate / gen generates new set of TLS configurations certs ros tls gen    --hostname / -H the hostname for which you want to generate the certificate (default: localhost) ros tls gen --hostname localhost    --server / -s generate the server keys instead of client keys ros tls gen --server    --dir / -d the directory to save/read certs from/to ros tls gen --dir value    "});index.add({'id':58,'href':'/docs/storage/additional-mounts/','title':"Additional Mounts",'section':"Storage",'content':"Additional Mounts #  Additional mounts can be specified as part of your cloud-config. These mounts are applied within the console container. Here\u0026rsquo;s a simple example that mounts /dev/vdb to /mnt/s.\n#cloud-config mounts: - [\u0026#34;/dev/vdb\u0026#34;, \u0026#34;/mnt/s\u0026#34;, \u0026#34;ext4\u0026#34;, \u0026#34;\u0026#34;]  Important: Be aware, the 4th parameter is mandatory and cannot be omitted (server crashes). It also yet cannot be defaults\n As you will use the ros cli most probably, it would look like this:\nros config set mounts \u0026#39;[[\u0026#34;/dev/vdb\u0026#34;,\u0026#34;/mnt/s\u0026#34;,\u0026#34;ext4\u0026#34;,\u0026#34;\u0026#34;]]\u0026#39;  Note: You need to pre-format the disks, BurmillaOS will not do this for you. The mount will not work (silently) until you formatted the disk or enabled autoformatting.\n mkfs.ext4 /dev/vdb The four arguments for each mount are the same as those given for cloud-init. Only the first four arguments are currently supported. The mount_default_fields key is not yet implemented.\nBurmillaOS uses the mount syscall rather than the mount command behind the scenes. This means that auto cannot be used as the filesystem type (third argument) and defaults cannot be used for the options (forth argument).\nShared Mounts #  By default, /media and /mnt are mounted as shared in the console container. This means that mounts within these directories will propagate to the host as well as other system services that mount these folders as shared.\nSee here for a more detailed overview of shared mounts and their properties.\n"});index.add({'id':59,'href':'/docs/storage/state-partition/','title':"State Partition",'section':"Storage",'content':"Persistent State Partition #  BurmillaOS will store its state in a single partition specified by the dev field. The field can be a device such as /dev/sda1 or a logical name such LABEL=state or UUID=123124. The default value is LABEL=RANCHER_STATE. The file system type of that partition can be set to auto or a specific file system type such as ext4.\n#cloud-config rancher: state: fstype: auto dev: LABEL=RANCHER_STATE For other labels such as RANCHER_BOOT and BURMILLA_OEM and BURMILLA_SWAP, please refer to Custom partition layout.\nAutoformat #  You can specify a list of devices to check to format on boot. If the state partition is already found, BurmillaOS will not try to auto format a partition. By default, auto-formatting is off.\nBurmillaOS will autoformat the partition to ext4 (not what is set in fstype) if the device specified in autoformat:\n Contains a boot2docker magic string Starts with 1 megabyte of zeros and rancher.state.formatzero is true  #cloud-config rancher: state: autoformat: - /dev/sda - /dev/vda "});index.add({'id':60,'href':'/docs/system-services/custom-system-services/','title':"Custom System Services",'section':"System Services",'content':"Custom System Services #  You can also create your own system service in Docker Compose format. After creating your own custom service, you can launch it in BurmillaOS in a couple of methods. The service could be directly added to the cloud-config, or a docker-compose.yml file could be saved at a http(s) url location or in a specific directory of BurmillaOS.\nLaunching Services #  Using Cloud-Config #  If you want to boot BurmillaOS with a system service running, you can add the service to the cloud-config that is passed to BurmillaOS. When BurmillaOS starts, this service will automatically be started.\n#cloud-config rancher: services: nginxapp: image: nginx restart: always Using Local Files #  If you already have BurmillaOS running, you can start a system service by saving a docker-compose.yml file at /var/lib/rancher/conf/.\nnginxapp: image: nginx restart: always To enable a custom system service from the file location, the command must indicate the file location if saved in BurmillaOS. If the file is saved at a http(s) url, just use the http(s) url when enabling/disabling.\n# Enable the system service saved in /var/lib/rancher/conf $ sudo ros service enable /var/lib/rancher/conf/example.yml # Enable a system service saved at a http(s) url $ sudo ros service enable https://mydomain.com/example.yml After the custom system service is enabled, you can start the service using sudo ros service up \u0026lt;serviceName\u0026gt;. The \u0026lt;serviceName\u0026gt; will be the names of the services inside the docker-compose.yml.\n$ sudo ros service up nginxapp # If you have more than 1 service in your docker-compose.yml, add all service names to the command $ sudo ros service up service1 service2 service3 Using a Web Repository #  The https://github.com/burmilla/os-services repository is used for the built-in services, but you can create your own, and configure BurmillaOS to use it in addition (or to replace) it.\nThe config settings to set the url in which ros should look for an index.yml file is: rancher.repositories.\u0026lt;name\u0026gt;.url. The core repository url is set when a release is made, and any other \u0026lt;name\u0026gt; url you add will be listed together when running ros console list, ros service list or ros engine list\nFor example, in BurmillaOS v0.7.0, the core repository is set to https://raw.githubusercontent.com/burmilla/os-services/v0.7.0.\nExisting Services #  Cron #  Available as of RancherOS v1.1\nBurmillaOS has a system cron service based on Container Crontab. This can be used to start, restart or stop system containers.\nTo use this on your service, add a cron.schedule label to your service\u0026rsquo;s description:\nmy-service: image: namespace/my-service:v1.0.0 command: my-command labels: io.rancher.os.scope: \u0026quot;system\u0026quot; cron.schedule: \u0026quot;0 * * * * ?\u0026quot; For a cron service that can be used with user Docker containers, see the crontab system service.\nLog rotation #  BurmillaOS provides a built in logrotate container that makes use of logrotate(8) to rotate system logs. This is called on an hourly basis by the system-cron container.\nIf you would like to make use of system log rotation for your system service, do the following.\nAdd system-volumes to your service description\u0026rsquo;s volumes_from section. You could also use a volume group containing system-volumes e.g. all-volumes.\nmy-service: image: namespace/my-service:v1.0.0 command: my-command labels: io.rancher.os.scope: \u0026quot;system\u0026quot; volumes_from: - system-volumes Next, add an entry point script to your image and copy your logrotate configs to /etc/logrotate.d/ on startup.\nExample Dockerfile:\nFROM alpine:latest COPY logrotate-myservice.conf entrypoint.sh / ENTRYPOINT [\u0026quot;/entrypoint.sh\u0026quot;] Example entrypoint.sh (Ensure that this script has the execute bit set).\n#!/bin/sh cp logrotate-myservice.conf /etc/logrotate.d/myservice exec \u0026quot;$@\u0026quot; Your service\u0026rsquo;s log rotation config will now be included when the system logrotate runs. You can view logrotate output with system-docker logs logrotate.\nDevelopment and Testing #  If you\u0026rsquo;re building your own services in a branch on GitHub, you can push to it, and then load your service from there.\nFor example, when developing the zfs service:\nrancher@zfs:~$ sudo ros config set rancher.repositories.zfs.url https://raw.githubusercontent.com/SvenDowideit/os-services/zfs-service rancher@zfs:~$ sudo ros service list disabled amazon-ecs-agent disabled kernel-extras enabled kernel-headers disabled kernel-headers-system-docker disabled open-vm-tools disabled amazon-ecs-agent disabled kernel-extras disabled kernel-headers disabled kernel-headers-system-docker disabled open-vm-tools disabled zfs [rancher@zfs ~]$ sudo ros service enable zfs Pulling zfs (zombie/zfs)... latest: Pulling from zombie/zfs b3e1c725a85f: Pull complete 4daad8bdde31: Pull complete 63fe8c0068a8: Pull complete 4a70713c436f: Pull complete bd842a2105a8: Pull complete d1a8c0826fbb: Pull complete 5f1c5ffdf34c: Pull complete 66c2263f2388: Pull complete Digest: sha256:eab7b8c21fbefb55f7ee311dd236acee215cb6a5d22942844178b8c6d4e02cd9 Status: Downloaded newer image for zombie/zfs:latest [rancher@zfs ~]$ sudo ros service up zfs WARN[0000] The KERNEL_VERSION variable is not set. Substituting a blank string. INFO[0000] Project [os]: Starting project INFO[0000] [0/21] [zfs]: Starting INFO[0000] [1/21] [zfs]: Started INFO[0000] Project [os]: Project started Beware that there is an overly aggressive caching of yml files - so when you push a new yml file to your repo, you need to delete the files in /var/lib/rancher/cache.\nThe image that you specify in the service yml file needs to be pullable - either from a private registry, or on the Docker Hub.\nCreating your own Console #  Once you have your own Services repository, you can add a new service to its index.yml, and then add a \u0026lt;service-name\u0026gt;.yml file to the directory starting with the first letter.\nTo create your own console images, you need to:\n install some basic tools, including an ssh daemon, sudo, and kernel module tools create rancher and docker users and groups with UID and GID\u0026rsquo;s of 1100 and 1101 respectively add both users to the docker and sudo groups add both groups into the /etc/sudoers file to allow password-less sudo configure sshd to accept logins from users in the docker group, and deny root. set ENTRYPOINT [\u0026quot;/usr/bin/ros\u0026quot;, \u0026quot;entrypoint\u0026quot;]  the ros binary, and other host specific configuration files will be bind mounted into the running console container when its launched.\nFor examples of existing images, see https://github.com/burmilla/os-images.\nLabels #  We use labels to determine how to handle the service containers.\n   Key Value Description     io.rancher.os.detach Default: true Equivalent of docker run -d. If set to false, equivalent of docker run --detach=false   io.rancher.os.scope system Use this label to have the container deployed in System Docker instead of Docker.   io.rancher.os.before/io.rancher.os.after Service Names (Comma separated list is accepted) Used to determine order of when containers should be started.   io.rancher.os.createonly Default: false When set to true, only a docker create will be performed and not a docker start.   io.rancher.os.reloadconfig Default: false When set to true, it reloads the configuration.    BurmillaOS uses labels to determine if the container should be deployed in System Docker. By default without the label, the container will be deployed in User Docker.\nlabels: - io.rancher.os.scope=system Example of how to order container deployment #  foo: labels: # Start foo before bar is launched io.rancher.os.before: bar # Start foo after baz has been launched io.rancher.os.after: baz "});index.add({'id':61,'href':'/docs/system-services/environment/','title':"Environment",'section':"System Services",'content':"Environment #  The environment key can be used to customize system services. When a value is not assigned, BurmillaOS looks up the value from the rancher.environment key.\nIn the example below, ETCD_DISCOVERY will be set to https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7 for the etcd service.\nrancher: environment: ETCD_DISCOVERY: https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7 services: etcd: ... environment: - ETCD_DISCOVERY Wildcard globbing is also supported. In the example below, ETCD_DISCOVERY will be set as in the previous example, along with any other environment variables beginning with ETCD_.\nrancher: environment: ETCD_DISCOVERY: https://discovery.etcd.io/d1cd18f5ee1c1e2223aed6a1734719f7 services: etcd: ... environment: - ETCD_* There is also a way to extend PATH environment variable, PATH or path can be set, and multiple values can be comma-separated. Note that need to reboot before taking effect.\nrancher: environment: path: /opt/bin,/home/rancher/bin "});index.add({'id':62,'href':'/docs/system-services/system-docker-volumes/','title':"System Docker Volumes",'section':"System Services",'content':"System Docker Volumes #  A few services are containers in created state. Their purpose is to provide volumes for other services.\nuser-volumes #  Provides user accessible persistent storage directories, used by console service:\n/home /opt /var/lib/kubelet If you want to change user-volumes, for example, add /etc/kubernetes directory:\n$ sudo ros config set rancher.services.user-volumes.volumes [/home:/home,/opt:/opt,/var/lib/kubelet:/var/lib/kubelet,/etc/kubernetes:/etc/kubernetes] $ sudo reboot Please note that after the restart, the new persistence directory can take effect.\ncontainer-data-volumes #  Provides docker storage directory, used by console service (and, indirectly, by docker)\n/var/lib/docker command-volumes #  Provides necessary command binaries (read-only), used by system services:\n/usr/bin/docker-containerd.dist /usr/bin/docker-containerd-shim.dist /usr/bin/docker-runc.dist /usr/bin/docker.dist /usr/bin/dockerlaunch /usr/bin/system-docker /sbin/poweroff /sbin/reboot /sbin/halt /sbin/shutdown /usr/bin/respawn /usr/bin/ros /usr/bin/cloud-init /usr/sbin/netconf /usr/sbin/wait-for-docker /usr/bin/switch-console system-volumes #  Provides necessary persistent directories, used by system services:\n/host/dev /etc/docker /etc/hosts /etc/resolv.conf /etc/ssl/certs/ca-certificates.crt.burmilla /etc/selinux /lib/firmware /lib/modules /run /usr/share/ros /var/lib/rancher/cache /var/lib/rancher/conf /var/lib/rancher /var/log /var/run all-volumes #  Combines all of the above, used by the console service.\n"});index.add({'id':63,'href':'/docs/installation/custom-builds/custom-burmillaos-iso/','title':"Custom BurmillaOS ISO",'section':"Custom Builds",'content':"Custom BurmillaOS ISO #  It\u0026rsquo;s easy to build your own BurmillaOS ISO.\nCreate a clone of the main BurmillaOS repository to your local machine with a git clone.\n$ git clone https://github.com/burmilla/os.git In the root of the repository, the \u0026ldquo;General Configuration\u0026rdquo; section of Dockerfile.dapper can be updated to use custom kernels. After you\u0026rsquo;ve saved your edits, run make in the root directory. After the build has completed, a ./dist/artifacts directory will be created with the custom built BurmillaOS release files. Build Requirements: bash, make, docker (Docker version \u0026gt;= 1.10.3)\n$ make $ cd dist/artifacts $ ls initrd burmillaos.iso iso-checksums.txt vmlinuz If you need a compressed ISO, you can run this command:\n$ make release The burmillaos.iso is ready to be used to boot BurmillaOS from ISO or launch BurmillaOS using Docker Machine.\nCreating a GCE Image Archive #  Create a clone of the main BurmillaOS repository to your local machine with a git clone.\n$ git clone https://github.com/burmilla/os-packer.git GCE supports KVM virtualization, and we use packer to build KVM images. Before building, you need to verify that the host can support KVM. If you want to build GCE image based on BurmillaOS v1.4.0, you can run this command:\nBURMILLAOS_VERSION=v1.4.0 make build-gce Custom Build Cases #  Reduce Memory Requirements #  With changes to the kernel and built Docker, BurmillaOS booting requires more memory. For details, please refer to the memory requirements.\nBy customizing the ISO, you can reduce the memory usage on boot. The easiest way is to downgrade the built-in Docker version, because Docker takes up a lot of space. This can effectively reduce the memory required to decompress the initrd on boot. Using docker 17.03 is a good choice:\n# run make $ USER_DOCKER_VERSION=17.03.2 make release Building with a Different Console #  Available as of RancherOS v1.5.0\nWhen building BurmillaOS, you have the ability to automatically start in a supported console instead of booting into the default console and switching to your desired one.\nHere is an example of building BurmillaOS and having the alpine console enabled:\n$ OS_CONSOLE=alpine make release Building with Predefined Docker Images #  If you want to use a custom ISO file to address an offline scenario, you can use predefined images for system-docker and user-docker.\nBurmillaOS supports APPEND_SYSTEM_IMAGES. It can save images to the initrd file, and is loaded with system-docker when booting.\nYou can build the ISO like this:\nAPPEND_SYSTEM_IMAGES=\u0026#34;burmilla/os-openvmtools:10.3.10-1\u0026#34; make release BurmillaOS also supports APPEND_USER_IMAGES. It can save images to the initrd file, and is loaded with user-docker when booting.\nYou can build the ISO like this:\nAPPEND_USER_IMAGES=\u0026#34;alpine:3.9 ubuntu:bionic\u0026#34; make release Please note that these will be packaged into the initrd, and the predefined images will affect the resource footprint at startup.\n"});index.add({'id':64,'href':'/docs/networking/dns/','title':"DNS",'section':"Networking",'content':"Configuring DNS #  If you wanted to configure the DNS through the cloud config file, you\u0026rsquo;ll need to place DNS configurations within the rancher key.\n#cloud-config #Remember, any changes for rancher will be within the rancher key rancher: network: dns: search: - mydomain.com - example.com Using ros config, you can set the nameservers, and search, which directly map to the fields of the same name in /etc/resolv.conf.\n$ sudo ros config set rancher.network.dns.search \u0026#34;[\u0026#39;mydomain.com\u0026#39;,\u0026#39;example.com\u0026#39;]\u0026#34; $ sudo ros config get rancher.network.dns.search - mydomain.com - example.com "});index.add({'id':66,'href':'/docs/installation/cloud/gce/','title':"Google Compute Engine (GCE)",'section':"Cloud",'content':"Google Compute Engine (GCE) #   Note: Due to the maximum transmission unit (MTU) of 1460 bytes on GCE, you will need to configure your network interfaces and both the Docker and System Docker to use a MTU of 1460 bytes or you will encounter weird networking related errors.\n Adding the BurmillaOS Image into GCE #  BurmillaOS is available as an image in GCE, and can be easily run in Google Compute Engine (GCE). Let’s walk through how to upload GCE image.\n Download the most recent BurmillaOS image. The image can be found in the release artifacts. It is a .tar.gz file. Follow Google\u0026rsquo;s instructions on how to upload the image. The image must be uploaded into a Google Cloud Storage bucket before it can be added to a project. Follow Google\u0026rsquo;s instructions on how to import a RAW image. Once the image is added to your Google Compute Engine, we can start creating new instances!  Launching BurmillaOS using gcloud compute #  After the image is uploaded, we can use the gcloud compute command-line tool to start a new instance. It automatically merges the SSH keys from the project and adds the keys to the rancher user. If you don\u0026rsquo;t have any project level SSH keys, go to the Adding SSH Keys section to learn more about adding SSH keys.\nSince the image is private, we need to follow Google\u0026rsquo;s instructions.\n$ gcloud compute instances create --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_TO_CREATE_INSTANCE\u0026gt; \u0026lt;INSTANCE_NAME\u0026gt; --image \u0026lt;PRIVATE_IMAGE_NAME\u0026gt; Using a Cloud Config File with GCE #  If you want to pass in your own cloud config file that will be processed by cloud init, you can pass it as metadata upon creation of the instance during the gcloud compute command. The file will need to be stored locally before running the command. The key of the metadata will be user-data and the value is the location of the file. If any SSH keys are added in the cloud config file, it will also be added to the rancher user.\n$ gcloud compute instances create --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_TO_CREATE_INSTANCE\u0026gt; \u0026lt;INSTANCE_NAME\u0026gt; --image \u0026lt;PRIVATE_IMAGE_NAME\u0026gt; --metadata-from-file user-data=/Directory/of/Cloud_Config.yml Adding your Cloud Config to Existing Instance #  If you have already created the instance, you can still add the cloud config file after the instance is created. You will just need to reset the machine after you\u0026rsquo;ve added the metadata.\n$ gcloud compute instances add-metadata \u0026lt;INSTANCE_NAME\u0026gt; --metadata-from-file user-data=/Directory/of/File --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_OF_INSTANCE\u0026gt; Updated [https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE_OF_INSTANCE/instances/INSTANCE_NAME]. $ gcloud compute instances reset \u0026lt;INSTANCE_NAME\u0026gt; --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_OF_INSTANCE\u0026gt; Updated [https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE_OF_INSTANCE/instances/INSTANCE_NAME]. Reviewing your Cloud Config #  If you want to review the cloud config file for your instance, review the metadata section:\n$ gcloud compute instances describe \u0026lt;INSTANCE_NAME\u0026gt; --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_OF_INSTANCE\u0026gt; Removing your Cloud Config #  If you want to remove your cloud config file, use the following command to remove the metadata.\n$ gcloud compute instances remove-metadata \u0026lt;INSTANCE_NAME\u0026gt; --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_OF_INSTANCE\u0026gt; --keys user-data Updated [https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE_OF_INSTANCE/instances/INSTANCE_NAME]. Resetting your Instance #  After any changes to the cloud config file, you\u0026rsquo;ll need to reset the machine. You can reset either using the console or using this command:\n$ gcloud compute instances reset \u0026lt;INSTANCE_NAME\u0026gt; --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_OF_INSTANCE\u0026gt; Updated [https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE_OF_INSTANCE/instances/INSTANCE_NAME]. Launching BurmillaOS using the Google Console #  After the image is uploaded, it\u0026rsquo;s easy to use the console to create new instances. You will not be able to upload your own cloud config file when creating instances through the console. You can add it after the instance is created using gcloud compute commands and resetting the instance.\n Make sure you are in the project that the image was created in.  In the navigation bar, click on the VM instances, which is located at Compute -\u0026gt; Compute Engine -\u0026gt; Metadata. Click on Create instance.  Fill out the information for your instance. In the Image dropdown, your private image will be listed among the public images provided by Google. Select the private image for BurmillaOS. Click Create.  Your instance is being created and will be up and running shortly!  Adding SSH keys #  In order to SSH into the GCE instance, you will need to have SSH keys set up in either the project instance, add them to the instance after the instance is created, or add them using the gcloud compute commands to add meta-data to an instance.\nOption 1: Project Level SSH Keys\nIn your project, click on Metadata, which is located within Compute -\u0026gt; Compute Engine -\u0026gt; Metadata. Click on SSH Keys.\n Add the SSH keys that you want to have access to any instances within your project.\nNote: If you do this after any BurmillaOS instance is created, you will need to reset the instance so that the SSH keys are added to the rancher user.\nOption 2: Instance Level SSH Keys\nAfter your instance is created, click on the instance name. Scroll down to the SSH Keys section and click on Add SSH key. This key will only be applicable to the instance.\n After the SSH keys have been added, you\u0026rsquo;ll need to reset the machine, by clicking Reset.\n After a little bit, you will be able to SSH into the box using the rancher user.\nOption 3: Using the Cloud Config file\nYou can add SSH keys by adding them into the cloud config file. Follow the directions above that walk through how to pass the cloud config file to an instance.\nExample of cloud config file that has only SSH keys:\n#cloud-config ssh_authorized_keys: - ssh-rsa AAA... user@host Logging into BurmillaOS #   Remember, the SSH keys are passed to the rancher user. The SSH keys can be passed from the project level, the instance level or through the cloud config file. If you add any of these SSH keys after the instance has been created, the instance will need to be reset before the SSH keys are passed through.\n$ gcloud compute ssh rancher@\u0026lt;INSTANCE_NAME\u0026gt; --project \u0026lt;PROJECT_ID\u0026gt; --zone \u0026lt;ZONE_OF_INSTANCE\u0026gt; If you have issues logging into BurmillaOS, try using this command to help debug the instance.\n$ gcloud compute instances get-serial-port-output \u0026lt;INSTANCE_NAME\u0026gt; --zone \u0026lt;ZONE_OF_INSTANCE\u0026gt; --project \u0026lt;PROJECT_ID\u0026gt; "});index.add({'id':67,'href':'/docs/installation/server/pxe/','title':"iPXE",'section':"Server",'content':"iPXE #  #!ipxe # Boot a persistent BurmillaOS to RAM # Location of Kernel/Initrd images set base-url \u0026lt;url\u0026gt; kernel ${base-url}/vmlinuz rancher.state.dev=LABEL=RANCHER_STATE rancher.state.autoformat=[/dev/sda] rancher.state.wait rancher.cloud_init.datasources=[url:http://example.com/cloud-config] initrd ${base-url}/initrd boot If you want to autoformat the disk when booting by iPXE, you should add the rancher.state.autoformat part to kernel cmdline. However, this does not install the bootloader to disk, so you cannot upgrade BurmillaOS.\nIf you don\u0026rsquo;t add rancher.state.autoformat, BurmillaOS will run completely in memory, you can execute ros install to install to disk.\nHiding sensitive kernel commandline parameters #  Available as of RancherOS v0.9\nSecrets can be put on the kernel parameters line afer a -- double dash, and they will be not be shown in any /proc/cmdline. These parameters will be passed to the BurmillaOS init process and stored in the root accessible /var/lib/rancher/conf/cloud-init.d/init.yml file, and are available to the root user from the ros config commands.\nFor example, the kernel line above could be written as:\nkernel ${base-url}/vmlinuz rancher.state.dev=LABEL=RANCHER_STATE rancher.state.autoformat=[/dev/sda] -- rancher.cloud_init.datasources=[url:http://example.com/cloud-config] The hidden part of the command line can be accessed with either sudo ros config get rancher.environment.EXTRA_CMDLINE, or by using a service file\u0026rsquo;s environment array.\nAn example service.yml file:\ntest: image: alpine command: echo \u0026#34;tell me a secret ${EXTRA_CMDLINE}\u0026#34; labels: io.rancher.os.scope: system environment: - EXTRA_CMDLINE When this service is run, the EXTRA_CMDLINE will be set.\ncloud-init Datasources #  Valid cloud-init datasources for BurmillaOS.\n   type default     ec2 Default metadata address   digitalocean Default metadata address   packet Default metadata address   cloudstack Default metadata address   aliyun Default metadata address   gce Default metadata address   file Path   cmdline Kernel command line: cloud-config-url=http://link/user_data   configdrive /media/config-2   url URL address   vmware Set guestinfo cloud-init or interface data as per VMware ESXi   * This will add [\u0026ldquo;configdrive\u0026rdquo;, \u0026ldquo;vmware\u0026rdquo;, \u0026ldquo;ec2\u0026rdquo;, \u0026ldquo;digitalocean\u0026rdquo;, \u0026ldquo;packet\u0026rdquo;, \u0026ldquo;gce\u0026rdquo;] into the list of datasources to try    The vmware datasource was added as of v1.1.\nCloud-Config #  When booting via iPXE, BurmillaOS can be configured using a cloud-config file.\n"});index.add({'id':68,'href':'/docs/installation/cloud/openstack/','title':"OpenStack",'section':"Cloud",'content':"OpenStack #  BurmillaOS releases include an Openstack image that can be found on our releases page. The image format is QCOW3 that is backward compatible with QCOW2.\nWhen launching an instance using the image, you must enable Advanced Options -\u0026gt; Configuration Drive and in order to use a cloud-config file.\n"});index.add({'id':69,'href':'/docs/networking/proxy-settings/','title':"Proxy Settings",'section':"Networking",'content':"Configuring Proxy Settings #  HTTP proxy settings can be set directly under the network key. This will automatically configure proxy settings for both Docker and System Docker.\n#cloud-config rancher: network: http_proxy: https://myproxy.example.com https_proxy: https://myproxy.example.com no_proxy: localhost,127.0.0.1  Note: System Docker proxy settings will not be applied until after a reboot.\n To add the HTTP_PROXY, HTTPS_PROXY, and NO_PROXY environment variables to a system service, specify each under the environment key for the service.\n#cloud-config rancher: services: myservice: ... environment: - HTTP_PROXY - HTTPS_PROXY - NO_PROXY "});index.add({'id':70,'href':'/docs/storage/using-zfs/','title':"Using ZFS",'section':"Storage",'content':"Using ZFS #  Installing the ZFS service #  The zfs service will install the kernel-headers for your kernel (if you build your own kernel, you\u0026rsquo;ll need to replicate this service), and then download the ZFS on Linux source, and build and install it. Then it will build a zfs-tools image that will be used to give you access to the zfs tools.\nThe only restriction is that you must mount your zpool into /mnt, as this is the only shared mount directory that will be accessible throughout the system-docker managed containers (including the console).\n$ sudo ros service enable zfs $ sudo ros service up zfs # you can follow the progress of the build by running the following command in another ssh session: $ sudo ros service logs --follow zfs # wait until the build is finished. $ lsmod | grep zfs  Note: if you switch consoles, you may need to re-run sudo ros service up zfs.\n Creating ZFS pools #  After it\u0026rsquo;s installed, it should be ready to use. Make a zpool named zpool1 using a device that you haven\u0026rsquo;t yet partitioned (you can use sudo fdisk -l to list all the disks and their partitions).\n Note: You need to mount the zpool in /mnt to make it available to your host and in containers.\n $ sudo zpool list $ sudo zpool create zpool1 -m /mnt/zpool1 /dev/\u0026lt;some-disk-dev\u0026gt; $ sudo zpool list $ sudo zfs list $ sudo cp /etc/* /mnt/zpool1 $ docker run --rm -it -v /mnt/zpool1/:/data alpine ls -la /data To experiment with ZFS, you can create zpool backed by just ordinary files, not necessarily real block devices. In fact, you can mix storage devices in your ZFS pools; it\u0026rsquo;s perfectly fine to create a zpool backed by real devices and ordinary files.\nUsing the ZFS debugger utility #  The zdb command may be used to display information about ZFS pools useful to diagnose failures and gather statistics. By default the utility tries to load pool configurations from /etc/zfs/zpool.cache. Since the BurmillaOS ZFS service does not make use of the ZFS cache file and instead detects pools by inspecting devices, the zdb utility has to be invoked with the -e flag.\nE.g. to show the configuration for the pool zpool1 you may run the following command:\n$ sudo zdb -e -C zpool1 ZFS storage for Docker on BurmillaOS #  First, you need to stop thedocker system service and wipe out /var/lib/docker folder:\n$ sudo system-docker stop docker $ sudo rm -rf /var/lib/docker/* To enable ZFS as the storage driver for Docker, you\u0026rsquo;ll need to create a ZFS filesystem for Docker and make sure it\u0026rsquo;s mounted.\n$ sudo zfs create zpool1/docker $ sudo zfs list -o name,mountpoint,mounted At this point you\u0026rsquo;ll have a ZFS filesystem created and mounted at /zpool1/docker. According to Docker ZFS storage docs, if the Docker root dir is a ZFS filesystem, the Docker daemon will automatically use zfs as its storage driver.\nNow you\u0026rsquo;ll need to remove -s overlay (or any other storage driver) from the Docker daemon args to allow docker to automatically detect zfs.\n$ sudo ros config set rancher.docker.storage_driver \u0026#39;zfs\u0026#39; $ sudo ros config set rancher.docker.graph /mnt/zpool1/docker # Now that you\u0026#39;ve changed the Docker daemon args, you\u0026#39;ll need to start Docker $ sudo system-docker start docker After customizing the Docker daemon arguments and restarting docker system service, ZFS will be used as Docker storage driver:\n$ docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 1.12.6 Storage Driver: zfs Zpool: error while getting pool information strconv.ParseUint: parsing \u0026#34;\u0026#34;: invalid syntax Zpool Health: not available Parent Dataset: zpool1/docker Space Used By Parent: 19456 Space Available: 8256371200 Parent Quota: no Compression: off Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: host bridge null overlay Swarm: inactive Runtimes: runc Default Runtime: runc Security Options: seccomp Kernel Version: 4.9.6-burmilla Operating System: BurmillaOS v0.8.0-rc8 OSType: linux Architecture: x86_64 CPUs: 1 Total Memory: 1.953 GiB Name: ip-172-31-24-201.us-west-1.compute.internal ID: IEE7:YTUL:Y3F5:L6LF:5WI7:LECX:YDB5:LGWZ:QRPN:4KDI:LD66:KYTC Docker Root Dir: /mnt/zpool1/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Insecure Registries: 127.0.0.0/8 "});index.add({'id':71,'href':'/docs/installation/cloud/vmware-esxi/','title':"VMware ESXi",'section':"Cloud",'content':"VMware ESXi #  BurmillaOS automatically detects that it is running on VMware ESXi, and automatically adds the open-vm-tools service to be downloaded and started, and uses guestinfo keys to set the cloud-init data.\nBurmillaOS releases anything required for VMware, which includes initrd, a standard ISO for VMware, a vmdk image, and a specific ISO to be used with Docker Machine. The open-vm-tools is built in to BurmillaOS, there is no need to download it.\nYou can download those files from https://github.com/burmilla/os/releases/\n   Description File name     Booting from ISO burmillaos.iso   For docker-machine burmillaos-autoformat.iso   VMDK burmillaos.vmdk   Initrd initrd    VMware Guest Info #     VARIABLE TYPE     hostname hostname   interface.\u0026lt;n\u0026gt;.name string   interface.\u0026lt;n\u0026gt;.mac MAC address (is used to match the ethernet device\u0026rsquo;s MAC address, not to set it)   interface.\u0026lt;n\u0026gt;.dhcp {\u0026ldquo;yes\u0026rdquo;, \u0026ldquo;no\u0026rdquo;}   interface.\u0026lt;n\u0026gt;.role {\u0026ldquo;public\u0026rdquo;, \u0026ldquo;private\u0026rdquo;}   interface.\u0026lt;n\u0026gt;.ip.\u0026lt;m\u0026gt;.address CIDR IP address   interface.\u0026lt;n\u0026gt;.route.\u0026lt;l\u0026gt;.gateway IP address   interface.\u0026lt;n\u0026gt;.route.\u0026lt;l\u0026gt;.destination CIDR IP address (not available yet)   dns.server.\u0026lt;x\u0026gt; IP address   dns.domain.\u0026lt;y\u0026gt; DNS search domain   cloud-init.config.data string   cloud-init.data.encoding {\u0026quot;\u0026quot;, \u0026ldquo;base64\u0026rdquo;, \u0026ldquo;gzip+base64\u0026rdquo;}   cloud-init.config.url URL     Note: \u0026ldquo;n\u0026rdquo;, \u0026ldquo;m\u0026rdquo;, \u0026ldquo;l\u0026rdquo;, \u0026ldquo;x\u0026rdquo; and \u0026ldquo;y\u0026rdquo; are 0-indexed, incrementing integers. The identifier for an interface (\u0026lt;n\u0026gt;) is used in the generation of the default interface name in the form eth\u0026lt;n\u0026gt;.\n "});})();